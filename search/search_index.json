{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Convert-Pheno Documentation","text":"<p>Welcome to the documentation for Convert-Pheno!</p> <p>Disclaimer</p> <p>This software is fully functional but still in beta, with ongoing improvements based on feedback from its use in several European-funded projects.</p> <p>This documentation provides instructions and guidelines for installing and using a software tool specifically designed for converting standard phenotypic data models. It includes detailed explanations and instructions to ensure that the software is implemented properly and functions optimally.</p>"},{"location":"about/","title":"\ud83d\udcc4 About","text":"<p>Convert-Pheno has been developed at CNAG, Barcelona, Spain.</p>"},{"location":"about/#developers","title":"Developers","text":"CLI - Module - APIWeb App User InterfaceDocumentation <ul> <li>Manuel Rueda</li> </ul> <ul> <li>Ivo C. Leist</li> </ul> <ul> <li>Manuel Rueda</li> <li>Ivo C. Leist - UI</li> </ul> <p>Project documentation was created using Material for MkDocs.</p>"},{"location":"about/#acknowledgments","title":"Acknowledgments","text":"CNAGExternal <ul> <li>Ivo G. Gut</li> <li>Davide Piscia and his team</li> <li>Sofia Chaves - Design and UX</li> </ul> <ul> <li>Florian Tran (Christian Albrechts University Kiel) - REDCap / CDISC-ODM</li> <li>Ioannis Parodis (Karolinska Institutet) - REDCap</li> <li>Juan Manuel Ramirez and M. A. Mayer (IMIM) - OMOP CDM</li> <li>Alberto Labarga and Sergi Aguilo (BSC) - OMOP CDM</li> </ul>"},{"location":"bff/","title":"\ud83e\uddec Beacon v2 Models (BFF)","text":"<p>BFF stands for Beacon Friendly Format. The BFF is a data exchange format composed of 7 <code>JSON</code> files. These files correspond to the 7 entities of the Beacon v2 Models.</p> <p> </p> Entities in Beacon v2 Models <p>About Beacon v2 Models' entities</p> <p>Among the seven entity types (also known as entry types) in the Beacon v2 Models, the individuals entity is typically the only one that contains phenotypic data, represented with record-level granularity. Other entities, such as datasets and biosamples, may also hold useful information. However, they generally pose fewer challenges for data conversion, primarily consisting of plain text with fewer nested properties.</p> <p>As an input, <code>Convert-Pheno</code> accepts data from the individuals entity, serialized in BFF format (<code>individuals.json</code>).</p> Browsing BFF <code>JSON</code> data <p>You can browse a public BFF v2 file with the following JSON viewers:</p> <ul> <li>JSON Crack</li> <li>JSON Hero</li> <li>Datasette</li> </ul>"},{"location":"bff/#bff-individuals-as-input","title":"BFF (individuals) as input","text":"Command-lineModuleAPI <p>When using the <code>convert-pheno</code> command-line interface, simply ensure the correct syntax is provided.</p> About <code>JSON</code> data in <code>individuals.json</code> <p>If the file <code>individuals.json</code> is a JSON array of objects (for which each object corresponds to an individual), the output <code>-opxf</code> file will also be a JSON array.</p> <pre><code>convert-pheno -ibff individuals.json -opxf phenopacket.json\n</code></pre> <p>The concept is to pass the necessary information as a hash (in Perl) or dictionary (in Python).</p> PerlPython <pre><code>$bff = {\n    data =&gt; $my_bff_json_data,\n    method =&gt; 'bff2pxf'\n};\n</code></pre> <pre><code>bff = {\n     \"data\" : my_bff_json_data,\n     \"method\" : \"bff2pxf\"\n}\n</code></pre> <p>The data will be sent as <code>POST</code> to the API's URL (see more info here). <pre><code>{\n\"data\": {...}\n\"method\": \"bff2pxf\"\n}\n</code></pre></p> <p>Please find below examples of data:</p> BFF (input)PXF (output) <pre><code>{\n  \"ethnicity\": {\n    \"id\": \"NCIT:C42331\",\n    \"label\": \"African\"\n  },\n  \"id\": \"HG00096\",\n  \"info\": {\n    \"eid\": \"fake1\"\n  },\n  \"interventionsOrProcedures\": [\n    {\n      \"procedureCode\": {\n        \"id\": \"OPCS4:L46.3\",\n        \"label\": \"OPCS(v4-0.0):Ligation of visceral branch of abdominal aorta NEC\"\n      }\n    }\n  ],\n  \"measures\": [\n    {\n      \"assayCode\": {\n        \"id\": \"LOINC:35925-4\",\n        \"label\": \"BMI\"\n      },\n      \"date\": \"2021-09-24\",\n      \"measurementValue\": {\n        \"quantity\": {\n          \"unit\": {\n            \"id\": \"NCIT:C49671\",\n            \"label\": \"Kilogram per Square Meter\"\n          },\n          \"value\": 26.63838307\n        }\n      }\n    },\n    {\n      \"assayCode\": {\n        \"id\": \"LOINC:3141-9\",\n        \"label\": \"Weight\"\n      },\n      \"date\": \"2021-09-24\",\n      \"measurementValue\": {\n        \"quantity\": {\n          \"unit\": {\n            \"id\": \"NCIT:C28252\",\n            \"label\": \"Kilogram\"\n          },\n          \"value\": 85.6358\n        }\n      }\n    },\n    {\n      \"assayCode\": {\n        \"id\": \"LOINC:8308-9\",\n        \"label\": \"Height-standing\"\n      },\n      \"date\": \"2021-09-24\",\n      \"measurementValue\": {\n        \"quantity\": {\n          \"unit\": {\n            \"id\": \"NCIT:C49668\",\n            \"label\": \"Centimeter\"\n          },\n          \"value\": 179.2973\n        }\n      }\n    }\n  ],\n  \"sex\": {\n    \"id\": \"NCIT:C20197\",\n    \"label\": \"male\"\n  }\n}\n</code></pre> <pre><code>{\n   \"diseases\" : [],\n   \"id\" : \"phenopacket_id.AUNb6vNX1\",\n   \"measurements\" : [\n      {\n         \"assay\" : {\n            \"id\" : \"LOINC:35925-4\",\n            \"label\" : \"BMI\"\n         },\n         \"value\" : {\n            \"quantity\" : {\n               \"unit\" : {\n                  \"id\" : \"NCIT:C49671\",\n                  \"label\" : \"Kilogram per Square Meter\"\n               },\n               \"value\" : 26.63838307\n            }\n         }\n      },\n      {\n         \"assay\" : {\n            \"id\" : \"LOINC:3141-9\",\n            \"label\" : \"Weight\"\n         },\n         \"value\" : {\n            \"quantity\" : {\n               \"unit\" : {\n                  \"id\" : \"NCIT:C28252\",\n                  \"label\" : \"Kilogram\"\n               },\n               \"value\" : 85.6358\n            }\n         }\n      },\n      {\n         \"assay\" : {\n            \"id\" : \"LOINC:8308-9\",\n            \"label\" : \"Height-standing\"\n         },\n         \"value\" : {\n            \"quantity\" : {\n               \"unit\" : {\n                  \"id\" : \"NCIT:C49668\",\n                  \"label\" : \"Centimeter\"\n               },\n               \"value\" : 179.2973\n            }\n         }\n      }\n   ],\n   \"medicalActions\" : [\n      {\n         \"procedure\" : {\n            \"code\" : {\n               \"id\" : \"OPCS4:L46.3\",\n               \"label\" : \"OPCS(v4-0.0):Ligation of visceral branch of abdominal aorta NEC\"\n            },\n            \"performed\" : {\n               \"timestamp\" : \"1900-01-01T00:00:00Z\"\n            }\n         }\n      }\n   ],\n   \"metaData\" : null,\n   \"subject\" : {\n      \"id\" : \"HG00096\",\n      \"sex\" : \"MALE\",\n      \"vitalStatus\" : {\n         \"status\" : \"ALIVE\"\n      }\n   }\n}\n</code></pre>"},{"location":"bff2pxf/","title":"\ud83d\udd04 BFF to PXF","text":"<p>BFF to PXF - Schemas</p> <ul> <li>Beacon v2 Models - individuals</li> <li>Phenopacket v2 schema</li> </ul> <p>Information</p> <p>The Phenopacket v2 schema enforces the presence of specific properties to achieve successful validation. We display in parenthesis the fields that are employed to guarantee conformity.</p>"},{"location":"bff2pxf/#version-025","title":"Version 0.25","text":""},{"location":"bff2pxf/#terms","title":"Terms","text":""},{"location":"bff2pxf/#id","title":"id","text":"BFF JSON path PXF JSON path (UNIQUE ID) id"},{"location":"bff2pxf/#subject","title":"subject","text":"BFF JSON path PXF JSON path id subject.id (ALIVE) subject.vitalStatus sex.label subject.sex info.dateOfBirth subject.dateOfBirth karyotypicSex subject.karyotypicSex"},{"location":"bff2pxf/#phenotypicfeatures","title":"phenotypicFeatures","text":"BFF JSON path PXF JSON path phenotypicFeatures.featureType phenotypicFeatures.type phenotypicFeatures.excluded phenotypicFeatures.excluded"},{"location":"bff2pxf/#measurements","title":"measurements","text":"BFF JSON path PXF JSON path measures.assayCode measurements.assay measures.measurementValue measurements.value measures.measurementValue.typedQuantities.quantityType measurements.complexValue.typedQuantities.type"},{"location":"bff2pxf/#biosamples","title":"biosamples","text":"BFF JSON path PXF JSON path info.biosamples biosamples"},{"location":"bff2pxf/#interpretations","title":"interpretations","text":"BFF JSON path PXF JSON path info.interpretations interpretations"},{"location":"bff2pxf/#diseases","title":"diseases","text":"BFF JSON path PXF JSON path diseases.diseaseCode diseases.term diseases.ageOfOnset diseases.onset"},{"location":"bff2pxf/#medicalactions","title":"medicalActions","text":"BFF JSON path PXF JSON path interventionsOrProcedures.procedureCode medicalActions.procedure.code interventionsOrProcedures.ageAtProcedure medicalActions.procedure.performed treatments.treatmentCode medicalActions.treatment.agent"},{"location":"bff2pxf/#files","title":"files","text":"<p>NA</p>"},{"location":"bff2pxf/#metadata","title":"metaData","text":"BFF JSON path PXF JSON path info.metaData metaData"},{"location":"bff2pxf/#exposures-not-listed-in-pxf-documentation","title":"exposures (not-listed in PXF documentation)","text":"BFF JSON path PXF JSON path exposures.exposureCode exposures.type exposures.date exposures.occurrence.timestamp"},{"location":"bff2pxf/#last-change-2023-03-09-by-manuel-rueda","title":"last change 2023-03-09 by Manuel Rueda","text":""},{"location":"cdisc-odm/","title":"\ud83d\udcd1 CDISC\u2011ODM","text":"<p>Experimental</p> <p>CDISC-ODM conversion is still experimental. Please us it with caution.</p> <p>CDISC stands for Clinical Data Interchange Standards Consortium. ODM stands for Operational Data Model.</p> <p> </p> Image extracted from www.cdisc.org <p>CDISC is an organization that develops standards for data exchange of clinical research. From their standards, we accept the Operational Data Model (ODM)-XML as input as it is widely used for representing forms for case-reporting in many electronic data capture (EDC) tools. </p> <p>About ODM-XML</p> <p>ODM-XML is a vendor-neutral, platform-independent format for exchanging and archiving clinical and translational research data, along with their associated metadata, administrative data, reference data, and audit information.</p>"},{"location":"cdisc-odm/#cdisc-odm-as-input","title":"CDISC-ODM as input","text":"Command-line <p>ODM versions</p> <p>We're accepting CDISC-ODM v1 (XML). Currently, v2 is in the process of being approved.</p> <p>We'll need three files:</p> <ol> <li>CDISC-ODM v1 (XML)</li> <li>REDCap data dictionary (CSV)</li> <li>Mapping file (YAML or JSON) (see tutorial)</li> </ol> <pre><code>convert-pheno -icdisc cdisc.xml --redcap-dictionary dictionary.csv --mapping-file mapping.yaml -obff individuals.json\n</code></pre> <p>About other CDISC data exchange standars</p> <p>We are aware of the Dataset-XML format (an extension of ODM-XML) and the emerging Dataset-JSON specification. While we do not currently support them, we are following their development with interest.</p>"},{"location":"citation/","title":"Citation for Convert-Pheno","text":"<p>Citation</p> <p>Rueda M, et al. Convert-Pheno: A software toolkit for the interconversion of standard data models for phenotypic data. J Biomed Inform. 2023 Nov 28:104558. doi: 10.1016/j.jbi.2023.104558. PMID: 38035971.</p> <p>Funding agencies</p> <ul> <li>This project has received funding from the Innovative Medicines Initiative 2 Joint Undertaking (JU) under grant agreement No 831434 (3TR). The JU receives support from the European Union\u2019s Horizon 2020 research and innovation programme and EFPIA.   Funding agencies </li> <li>Institutional support was from the Spanish Instituto de Salud Carlos III, Fondo de Investigaciones Sanitarias and cofunded with ERDF funds (PI19/01772). </li> <li>We acknowledge the institutional support of the Spanish Ministry of Science and Innovation through the Instituto de Salud Carlos III and the 2014\u20132020 Smart Growth Operating Program, to the EMBL partnership and institutional co-financing with the European Regional Development Fund (MINECO/FEDER, BIO2015-71792-P). </li> <li>We also acknowledge the support from the Generalitat de Catalunya through the Departament de Salut, Departament d\u2019Empresa i Coneixement.</li> </ul>"},{"location":"csv/","title":"\ud83d\udcca CSV","text":"<p>Experimental feature</p> <p>CSV conversion to BFF, PXF and OMOP CDM data exchange formats is still in the development phase. Please us it with caution.</p>"},{"location":"csv/#csv-with-clinical-data-as-input","title":"CSV with clinical data as input","text":"<p>Note</p> <p>This conversion method helps users who don't have the tools or expertise to transform their raw clinical data. It aims to convert essential fields needed for comparing data across studies.</p> <p>If you use our tool and identify areas for improvement, please contact us or create a GitHub issue. Thank you.</p> Command-lineAPI <p>When using the <code>convert-pheno</code> command-line interface, simply ensure the correct syntax is provided.</p> <p>CSV Separator Notice</p> <p>Please note that the default separator for CSV files is <code>;</code>. If your file uses a different character (e.g., <code>,</code> or <code>:</code>), please specify it using the <code>--sep</code> option.</p> <pre><code>convert-pheno -icsv clinical_data.csv --mapping-file clinical_data_mapping.yaml -obff individuals.json --sep ,\n</code></pre> <p>Please refer to the Convert-Pheno tutorial for more information.</p> <p>How do I convert other Beacon v2 Models entities?</p> <p>We recommend using the maintaned version of the original Beacon v2 Reference Implementation tools (beacon2-ri-tools).</p> <p>See examples:</p> InputOutput <ul> <li>CSV data</li> <li>Mapping file</li> </ul> <ul> <li>BFF</li> <li>PXF</li> </ul> <p>While it is technically possible to perform a transformation via the <code>Convert-Pheno</code> API, we don't think this is how most people will transform CSV files (due to the need of the mapping file). Therefore, we recommend using the command-line version.</p> Input CLI UI Module API Beacon v2 Models YES YES YES YES CDISC-ODM YES YES YES NO CSV YES NO YES NO Phenopackets v2 YES YES YES YES OMOP-CDM YES YES YES YES REDCap YES YES YES NO"},{"location":"download-and-installation/","title":"\u2b07\ufe0f Download & Installation","text":"<p>Compatibility</p> <p>The software <code>Convert-Pheno</code> can be installed locally on the following operating systems:</p> Operating System Supported Versions Linux All major distributions macOS macOS 10.14 (Mojave) and later Windows Windows Server OS <p>We provide several alternatives (containerized and non-containerized) for download and installation.</p> Which download method should I use? <p>It depends in which components you want to use and your fluency in performing Docker-based installations. Most people use the CLI.</p> Use case Method CLI 1 (CPAN) CLI (conda) 2 (CPAN in Conda env) API 4 or 5 (Docker) Web App UI Here"},{"location":"download-and-installation/#non-containerized","title":"Non-Containerized","text":"Method 1: From CPANMethod 2: From CPAN in a Conda environmentMethod 3: From Github <p>The core of software is a module implemented in <code>Perl</code> and it is available in the Comprehensive Perl Archive Network (CPAN). See the description here.</p> <p>With the CPAN distribution you get:</p> <ul> <li>Module</li> <li>CLI</li> </ul> <p>Linux: Required system-level libraries</p> <p>Before procesing with installation, we will need to install a few system level dependencies:</p> <ul> <li> <p><code>libbz2-dev:</code> This is the development library for bzip2, which is used for data compression.</p> </li> <li> <p><code>zlib1g-dev:</code> This is the development library for zlib, which is another data compression library.</p> </li> <li> <p><code>libperl-dev:</code> This package contains the headers and libraries necessary to compile C or C++ programs to link against the Perl library, enabling you to write Perl modules in C or C++.</p> </li> <li> <p><code>libssl-dev:</code> This package is part of OpenSSL, which provides secure socket layer (SSL) capabilities. For SSL/TLS related tasks in Perl, you can use modules such as IO::Socket::SSL or Net::SSLeay, but these modules also require OpenSSL to be installed on the system.</p> </li> </ul> <p>To install it, plese see this README.</p> <p>With the CPAN distribution you get:</p> <ul> <li>Module</li> <li>CLI</li> </ul> <p>With the non-containerized version from Github you get:</p> <ul> <li>Module</li> <li>CLI</li> <li>APIs</li> </ul> <p>Please follow the instructions provided in this README.</p>"},{"location":"download-and-installation/#step-1-install-miniconda","title":"Step 1: Install Miniconda","text":"<p>Instructions for x86_64</p> <p>The following instructions work for <code>amd64|x86_64</code> architectures. If you have a new Mac please use <code>amd64</code>.</p> <ol> <li> <p>Download the Miniconda installer for Linux with the following command:</p> <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n</code></pre> </li> <li> <p>Run the installer:</p> <pre><code>bash Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Follow the prompts on the installer screens.</p> </li> <li> <p>Close and re-open your terminal window for the installation to take effect.</p> </li> </ol>"},{"location":"download-and-installation/#step-2-set-up-channels","title":"Step 2: Set Up Channels","text":"<p>Once you have Conda installed, set up the channels. Bioconda depends on the <code>conda-forge</code> and <code>defaults</code> channel.</p> <p>Add bioconda channels with the following command:</p> <pre><code>conda config --add channels bioconda\n</code></pre> <p>Note: It's recommended to use a new Conda environment when installing new packages to avoid dependency conflicts. You can create and activate a new environment with the following commands:</p>"},{"location":"download-and-installation/#step-3-installation","title":"Step 3: Installation","text":"<pre><code>conda create -n myenv\nconda activate myenv\n</code></pre> <p>(Replace myenv with the name you want to give to your environment)</p> <p>Then you can to run the following commands:</p> <pre><code>conda install -c conda-forge gcc_linux-64 perl perl-app-cpanminus\n#conda install -c bioconda perl-mac-systemdirectory # (MacOS only)\ncpanm --notest Convert::Pheno\n</code></pre> <p>You can execute <code>Convert::Pheno</code> CLI  by typing:</p> <pre><code>convert-pheno --help\n</code></pre> <p>To deactivate:</p> <pre><code>conda deactivate -n myenv\n</code></pre>"},{"location":"download-and-installation/#optional-using-convertpheno-perl-module-in-python","title":"Optional: Using Convert::Pheno <code>Perl</code> module in <code>Python</code>","text":"<p>First we will download and install <code>PyPerler</code></p> <pre><code>git clone https://github.com/tkluck/pyperler\ncd pyperler\nmake install 2&gt; install.log\n</code></pre> <p>Now you should be able to execute this file:</p> <pre><code>~/miniconda3/envs/myenv/lib/perl5/site_perl/auto/share/dist/Convert-Pheno/ex/python.py\n</code></pre> <p>This is the expected output:</p> <pre><code>{\n\"id\": \"P0007500\",\n\"sex\": {\n    \"id\": \"NCIT:C16576\",\n    \"label\": \"Female\"\n      }\n}\n</code></pre> <p>Feel free to copy that file and use for your own purposes.</p>"},{"location":"download-and-installation/#containerized","title":"Containerized","text":"<p>With the containerized version you get:</p> <ul> <li>Module</li> <li>CLI</li> <li>APIs</li> </ul> Method 4: From Docker HubMethod 5: With Dockerfile <p>Please follow the instructions provided in this README.</p> <p>Please follow the instructions provided in this README.</p>"},{"location":"faq/","title":"\ud83e\udd14 FAQs","text":"<p>Frequently Asked Questions</p>"},{"location":"faq/#general","title":"General","text":"What does <code>Convert-Pheno</code> do? <p><code>Convert-Pheno</code> is an open-source software toolkit designed to interconvert common data models for phenotypic data. The software addresses the challenge of inconsistent data storage across various research centers by enabling seamless conversion between different data models like Beacon v2 Models, CDISC-ODM, OMOP-CDM, Phenopackets v2, and REDCap. This facilitates data sharing and integration, ultimately accelerating scientific progress and improving patient outcomes in precision medicine and public health.</p> Is <code>Convert-Pheno</code> free? <p>Yes. See the license.</p> Is <code>Convert-Pheno</code> or <code>Pheno-Convert</code>? <p>It's <code>Convert-Pheno</code>, for two reasons:</p> <ol> <li>The naming is inspired by the <code>convert</code> utility from ImageMagick.</li> <li>In related contexts, people refer to PhenoConvert as in PhenoCopy or PhenoConversion.</li> </ol> Is <code>Convert-Pheno</code> ready for use in production environments? <p>The software is fully functional and has been successfully used in several European-funded projects. However, it is still in beta, so ongoing improvements and refinements are to be expected.</p> If I use <code>Convert-Pheno</code> to convert my data to Beacon v2 Models, does this mean I have a Beacon v2? <p>I am afraid not. Beacon v2 is an API specification, and the Beacon v2 Models are merely a component of it. In order to light a Beacon v2, it is necessary to load the <code>JSON</code> files into a database and add an an API on top. Currently, it is advisable to utilize the Beacon v2 Reference Implementation which includes the database, the Beacon v2 API, and other necessary components.</p> <p>See below an example in how to integrate an OMOP CDM export from SQL with Beacon v2.</p> <p> Beacon v2 RI integration </p> What is the difference between Beacon v2 Models and Beacon v2? <p>Beacon v2 is a specification to build an API. The Beacon v2 Models define the format for the API's responses to queries regarding biological data. With the help of <code>Convert-Pheno</code>, data exchange text files (BFF) that align with this response format can be generated. By doing so, the BFF files can be integrated into a non-SQL database, such as MongoDB, without the API having to perform any additional data transformations internally.</p> Why are there so many clinical data standards? <p>The healthcare industry uses various data standards to meet diverse needs for data exchange, storage, and analysis, tailored for specific purposes like real-time clinical use or research. The abundance of standards also stems from a lack of communication and coordination among different organizations and stakeholders.</p> Are you planning in supporting other clinical data formats? <p>Afirmative, but it will depend on community adoption. Please check our roadmap for more information.</p> Are longitudinal data supported? <p>Although Beacon v2 and Phenopackets v2 allow for storing time information in some properties, there is currently no way to associate medical visits to properties. To address this:</p> <ul> <li> <p><code>omop2bff</code> -  we added an ad hoc property (_visit) to store medical visit information for longitudinal events in variables that have it (e.g., measures, observations, etc.).</p> </li> <li> <p><code>redcap2bff</code> - In REDCap, visit/event information is not stored at the record level. We added this information inside <code>info</code> property.</p> </li> </ul> <p>We raised this issue to the respective communities in the hope of a more permanent solution.</p> What is an \"ontology\" in Beacon v2 and Phenopacket v2 context? <p>In the context of Phenopackets and Beacon v2, the terms ontologyClass and ontologyTerm denote standardized identifiers derived from ontologies such as HPO or NCIt, and terminologies like LOINC or RxNorm. The use of \"ontology\" here is broad, covering both actual ontologies\u2014with their complex semantic relationships and inference abilities\u2014and classifications like LOINC and RxNorm, which, despite not fitting the strict definition of an ontology, serve similar purposes in data standardization.</p> I have a collection of PXF files encoded using HPO and ICD-10 terms, and I need to convert them to BFF format, but encoded in OMIM and SNOMED-CT terminologies. Can you assist me with this? <p>Neither Phenopacket v2 nor Beacon v2 prescribe the use of a specific ontology; they simply provide recommendations on their websites. Thereby, <code>Convert-Pheno</code> does not change the source ontology terms.</p> <p>Now, IMHO, it's generally easier to inter-convert ontology terms (it's just a mapping exercise) than to inter-convert data schemas...so here is that.</p> <p>Nota Bene:</p> <p>A standard that does enforce the use of an standardized vocabulary is OMOP CDM, you may wanna check it out.</p> What type of data validation is carried out? <p>Data validation</p> <p>To ensure the integrity and validity of converted outputs, we employ external validation tools during development and in unit tests. Specifically, we used the bff-tools validate for Beacon Friendly Format (BFF) and phenopacket-tools for Phenotype Exchange Format (PXF). These validators were instrumental in ensuring converted data adhere to the respective schemas and standards; for example, conversions were validated until the output was 100% compliant with the target schema. The same validation process is applied to Beacon v2 and OMOP CDM outputs. By preserving non-mapped variables where appropriate and applying rigorous validation, we aim to mitigate information loss and maximise fidelity of the converted data.</p> <p>Important: Convert-Pheno does not validate your input data. Input validation is out of scope for the software. If fields are missing or malformed, Convert-Pheno will handle these cases internally and apply default values where appropriate, but it will not verify that your source files are complete or correct. We therefore recommend validating and cleaning source files before conversion.</p> <p>See:</p> <ul> <li>bff-tools validate</li> <li>phenopacket-tools</li> <li>OMOP CSV Validator</li> </ul> What type of database search is carried out? About text similarity in database searches <p><code>Convert-Pheno</code> comes with several pre-configured ontology/terminology databases. It supports three types of label-based search strategies:</p> Error Handling for <code>CSV_XS ERROR: 2023 - EIQ - QUO character not allowed @ rec 1 pos 21 field 1</code> <p>This indicates a problem with the character used to separate data fields in your file. Our script automatically detects the separator based on the file extension (e.g., it expects commas for <code>.csv</code> files). However, discrepancies can arise if the actual data separator doesn't match the expected one based on the file extension.</p> Should I export my REDCap project as raw data or as labels for use with <code>Convert-Pheno</code>? <p>For use with <code>Convert-Pheno</code>, we recommend that you export your REDCap project as CSV / Microsoft Excel (raw data). It's important to include the corresponding dictionary file with your export. For detailed instructions on how to prepare your export correctly, refer to the Convert-Pheno tutorial.</p> <p> Example of REDCap export settings. Source: CDC <p>Additionally, when configuring your export settings, ensure that in the Additional report options, the option \"Combine checkbox options into single column of only the checked-off options\" is not selected.</p> <p> REDCap checkbox export settings </p> <p>If your data has been exported as CSV / Microsoft Excel (labels) you can use follow the CSV input route.</p>"},{"location":"faq/#last-change-2023-01-05-by-manuel-rueda","title":"last change 2023-01-05 by Manuel Rueda","text":""},{"location":"faq/#last-change-2023-01-04-by-manuel-rueda","title":"last change 2023-01-04 by Manuel Rueda","text":""},{"location":"faq/#last-change-2023-01-05-by-manuel-rueda_1","title":"last change 2023-01-05 by Manuel Rueda","text":""},{"location":"faq/#last-change-2023-06-27-by-manuel-rueda","title":"last change 2023-06-27 by Manuel Rueda","text":""},{"location":"faq/#last-change-2023-06-20-by-manuel-rueda","title":"last change 2023-06-20 by Manuel Rueda","text":""},{"location":"faq/#last-change-2023-02-13-by-manuel-rueda","title":"last change 2023-02-13 by Manuel Rueda","text":""},{"location":"faq/#overview-of-key-healthcare-data-standards-and-models","title":"Overview of Key Healthcare Data Standards and Models","text":"Standard/Model Purpose Data Persistence Live Data Use (Clinical Settings) Secondary Data Use (Research Settings) Beacon v2 Facilitates the discovery and sharing of genomic data, enabling researchers to find relevant genomic datasets across different repositories. Not designed for long-term storage; focuses on data discovery. No Yes CDISC-ODM Manages and archives clinical trial data, providing a standardized format for the exchange and submission of clinical research data. Strong support for long-term data archiving and regulatory submissions. No Yes HL7/CDA Standardizes the structure and semantics of clinical documents (such as discharge summaries and progress notes) for exchange. Ensures structured document storage; persistence depends on implementation. Yes Yes HL7/FHIR Facilitates the exchange of healthcare information electronically, supporting interoperability across different health IT systems. Provides guidelines for data exchange; persistence depends on implementation. Yes Yes OMOP CDM Standardizes and harmonizes health data for research and secondary use, focusing on observational health data analysis. Supports data persistence for research purposes, not real-time use. No Yes openEHR Offers a comprehensive standard for electronic health records, focusing on accurate, long-term clinical data storage and real-time use. Designed for robust, long-term clinical data persistence. Yes Yes Phenopackets v2 Standardizes the exchange of detailed phenotypic data, particularly for genetic and rare disease research. Not designed for long-term storage; focuses on data exchange. No Yes REDCap Provides a secure, web-based application for building and managing online surveys and databases, primarily used in research settings. Supports data persistence for research projects and surveys. No Yes Composite Similarity Score <p>The composite similarity score is computed as a weighted sum of two measures: the token-based similarity and the normalized Levenshtein similarity.</p>"},{"location":"faq/#last-change-2024-07-12-by-manuel-rueda","title":"last change 2024-07-12 by Manuel Rueda","text":""},{"location":"faq/#last-change-2023-01-04-by-manuel-rueda_1","title":"last change 2023-01-04 by Manuel Rueda","text":""},{"location":"faq/#last-change-2023-03-24-by-manuel-rueda","title":"last change 2023-03-24 by Manuel Rueda","text":""},{"location":"faq/#last-change-2024-04-01-by-manuel-rueda","title":"last change 2024-04-01 by Manuel Rueda","text":""},{"location":"faq/#last-change-2024-01-16-by-manuel-rueda","title":"last change 2024-01-16 by Manuel Rueda","text":""},{"location":"faq/#last-change-2025-09-26-by-manuel-rueda","title":"last change 2025-09-26 by Manuel Rueda","text":""},{"location":"faq/#1-exact-default","title":"1. <code>exact</code> (default)","text":"<p>Returns only exact matches for the given label string. If the label is not found exactly, no results are returned.</p>"},{"location":"faq/#2-mixed-use-search-mixed","title":"2. <code>mixed</code> (use <code>--search mixed</code>)","text":"<p>Hybrid search: First tries to find an exact label match. If none is found, it performs a token-based similarity search and returns the closest matching concept based on the highest similarity score.</p>"},{"location":"faq/#3-fuzzy-use-search-fuzzy","title":"3. \u2728 <code>fuzzy</code> (use <code>--search fuzzy</code>)","text":"<p>Hybrid search with fuzzy ranking: Like <code>mixed</code>, it starts with an exact match attempt. If that fails, it performs a weighted similarity search, where: - 90% of the score comes from token-based similarity (e.g., cosine or Dice coefficient), - 10% comes from the normalized Levenshtein similarity.</p> <p>The concept with the highest composite score is returned.</p> <p>Note: The normalized Levenshtein similarity is computed on top of the candidate results produced by the full text search. In this approach, an initial full text search (using token-based methods) returns a set of potential matches. The fuzzy search then refines these results by applying the normalized Levenshtein distance to better handle minor typographical differences, ensuring that the final composite score reflects both overall token similarity and fine-grained character-level differences.</p>"},{"location":"faq/#example-search-behavior","title":"\ud83d\udd0d Example Search Behavior","text":"<p>Query: <code>Exercise pain management</code> - With <code>--search exact</code>: \u2705 Match found \u2014 Exercise Pain Management</p> <p>Query: <code>Brain Hemorrhage</code> - With <code>--search mixed</code>:   - \u274c No exact match   - \u2705 Closest match by similarity: Intraventricular Brain Hemorrhage</p>"},{"location":"faq/#similarity-threshold","title":"\ud83d\udca1 Similarity Threshold","text":"<p>The <code>--min-text-similarity-score</code> option sets the minimum threshold for <code>mixed</code> and <code>fuzzy</code> searches. - Default: <code>0.8</code> (conservative) - Lowering the threshold may increase recall but may introduce irrelevant matches.</p>"},{"location":"faq/#performance-note","title":"\u26a0\ufe0f Performance Note","text":"<p>Both <code>mixed</code> and <code>fuzzy</code> modes are more computationally intensive and can produce unexpected or less interpretable matches. Use them with care, especially on large datasets.</p>"},{"location":"faq/#example-results-table","title":"\ud83e\uddea Example Results Table","text":"<p>Below is an example showing how the query <code>Sudden Death Syndrome</code> performs using different search modes against the NCIt ontology:</p> Query Search NCIt match (label) NCIt code Cosine Dice Levenshtein (Normalized) Composite Sudden Death Syndrome exact NA NA NA NA NA NA mixed CDISC SDTM Sudden Death Syndrome Type Terminology NCIT:C101852 0.65 0.60 NA NA Family History of Sudden Arrythmia Death Syndrome NCIT:C168019 0.65 0.60 NA NA Family History of Sudden Infant Death Syndrome NCIT:C168209 0.65 0.60 NA NA Sudden Infant Death Syndrome NCIT:C85173 0.86 0.86 NA NA \u2728 fuzzy CDISC SDTM Sudden Death Syndrome Type Terminology NCIT:C101852 0.65 0.60 0.43 0.63 Family History of Sudden Arrythmia Death Syndrome NCIT:C168019 0.65 0.60 0.43 0.63 Family History of Sudden Infant Death Syndrome NCIT:C168209 0.65 0.60 0.46 0.63 Sudden Infant Death Syndrome NCIT:C85173 0.86 0.86 0.75 0.85 <p>Interpretation: </p> <ul> <li> <p>With <code>exact</code>, there are no matches.</p> </li> <li> <p>With <code>mixed</code>, the best match will be <code>Sudden Infant Death Syndrome</code>.</p> </li> <li> <p>With <code>fuzzy</code>, the composite score (90% token-based + 10% Levenshtein similarity) is used to rank results.   The highest match is <code>Sudden Infant Death Syndrome</code>, with a composite score of 0.85.</p> </li> </ul> <p>\u2728 Now we introduce a typo on the query <code>Sudden Infant Deth Syndrome</code>:</p> Query Mode Candidate Label Code Cosine Dice Levenshtein (Normalized) Composite Sudden Infant Deth Syndrome fuzzy CDISC SDTM Sudden Death Syndrome Type Terminology NCIT:C101852 0.38 0.36 0.33 0.37 Family History of Sudden Arrythmia Death Syndrome NCIT:C168019 0.38 0.36 0.43 0.38 Family History of Sudden Infant Death Syndrome NCIT:C168209 0.57 0.55 0.59 0.57 Sudden Infant Death Syndrome NCIT:C85173 0.75 0.75 0.96 0.77 <p>To capture the best match we would need to lower the threshold to  <code>--min-text-similarity-score 0.75</code></p> <p>It is possible to change the weight of Levenshtein similarity via <code>--levenshtein-weight &lt;floating 0.0 - 1.0&gt;</code>.</p>"},{"location":"faq/#1-token-based-similarity","title":"1. Token-Based Similarity","text":"<p>This is calculated using methods like cosine or Dice similarity to measure how similar the tokens (words) of two strings are.</p>"},{"location":"faq/#2-normalized-levenshtein-similarity","title":"2. Normalized Levenshtein Similarity","text":"<p>The normalized Levenshtein similarity is defined as:</p> \\[ \\text{NormalizedLevenshtein}(s_1, s_2) = 1 - \\frac{\\text{lev}(s_1, s_2)}{\\max(|s_1|, |s_2|)} \\] <p>Where: - \\(\\text{lev}(s_1, s_2)\\) is the Levenshtein edit distance\u2014the minimum number of insertions, deletions, or substitutions required to change \\(s_1\\) into \\(s_2\\). - \\(|s_1|\\) and \\(|s_2|\\) are the lengths of the strings \\(s_1\\) and \\(s_2\\), respectively.</p> <p>This formula produces a score between 0 and 1, with 1.0 meaning identical strings and 0.0 meaning completely different strings.</p>"},{"location":"faq/#3-composite-score-formula","title":"3. Composite Score Formula","text":"<p>The final composite similarity score \\(C\\) is a weighted combination of the two metrics:</p> \\[ C(s_1, s_2) = \\alpha \\cdot \\text{TokenSimilarity}(s_1, s_2) + \\beta \\cdot \\text{NormalizedLevenshtein}(s_1, s_2) \\] <p>Where: - \\(\\alpha\\) (or <code>token_weight</code>) is the weight assigned to the token-based similarity. - \\(\\beta\\) (or <code>lev_weight</code>) is the weight assigned to the normalized Levenshtein similarity.</p> <p>A common default is to set \\(\\alpha = 0.9\\) and \\(\\beta = 0.1\\), emphasizing the token-based similarity. However, for short strings (4\u20135 words), you might consider adjusting the balance (for example, \\(\\alpha = 0.95\\) and \\(\\beta = 0.05\\)) if small typographical differences are less critical.</p>"},{"location":"faq/#last-change-2025-04-10-by-manuel-rueda","title":"last change 2025-04-10 by Manuel Rueda","text":""},{"location":"faq/#solutions","title":"Solutions","text":"<ul> <li> <p>Ensure Consistent Separator Use: If you're using REDCap for input, verify that both <code>--iredcap</code> and <code>--rcd</code> files are configured to use the identical separator. This consistency is crucial for correct data processing.</p> </li> <li> <p>Specify Separator Manually in Command Line: In cases where the default separator detection fails, you can manually specify the correct separator. For example, to use a tab as your separator, utilize the following syntax in the CLI:</p> </li> </ul> <pre><code>--sep $'\\t'\n</code></pre>"},{"location":"faq/#last-change-2024-02-06-by-manuel-rueda","title":"last change 2024-02-06 by Manuel Rueda","text":""},{"location":"faq/#last-change-2024-05-18-by-manuel-rueda","title":"last change 2024-05-18 by Manuel Rueda","text":""},{"location":"faq/#analytics","title":"Analytics","text":"How can I obtain statistics from the <code>individuals.json</code> file if I'm not familiar with <code>JSON</code> format? Any suggestions? <p>My first recommendation is to use <code>jq</code>, which is like <code>grep</code> for <code>JSON</code>.</p> <p>Let's begin by generating a <code>TSV</code> (Tab-Separated Values) file where each row represents an individual, and the columns correspond to the array variables:</p> <pre><code>jq -r '[\"id\", \"diseases\", \"exposures\", \"interventionsOrProcedures\", \"measures\", \"phenotypicFeatures\", \"treatments\"], (.[] | [.id, (.diseases | length), (.exposures | length), (.interventionsOrProcedures | length), (.measures | length), (.phenotypicFeatures | length), (.treatments | length)]) | @tsv' &lt; individuals.json &gt; results.tsv\n</code></pre> <p>Another valid option to acomplish the same task is to resort to a scripting language such as <code>Python</code> or <code>Perl</code>:</p> Python code <pre><code>import json\nimport pandas as pd\n\n# Load the JSON data from individuals.json\nwith open('individuals.json', 'r') as json_file:\n    data = json.load(json_file)\n\n# Define the keys you want to extract\nkeys = [ \"diseases\", \"exposures\", \"interventionsOrProcedures\", \"measures\", \"phenotypicFeatures\", \"treatments\"]\n\n# Create a list of dictionaries with the extracted values\nresult_data = [\n    {\n        \"id\": item[\"id\"],\n        **{key: len(item.get(key, [])) for key in keys}\n    }\n    for item in data\n]\n\n# Create a DataFrame from the list of dictionaries\ndf = pd.DataFrame(result_data)\n\n# Save the DataFrame to results.tsv with tab as the separator\ndf.to_csv('results.tsv', sep='\\t', index=False)\n</code></pre> Perl code <pre><code>use strict;\nuse warnings;\nuse autodie;\nuse JSON::XS;\nuse Text::CSV_XS qw(csv);\n\n# Open the JSON file and read the data\nopen my $json_file, '&lt;', 'individuals.json';\nmy $json_text = do { local $/; &lt;$json_file&gt; };\nmy $data = decode_json($json_text);\nclose $json_file;\n\n# Define the keys you want to extract\nmy @keys = (\"diseases\", \"exposures\", \"interventionsOrProcedures\", \"measures\", \"phenotypicFeatures\", \"treatments\");\n\n# Initialize the data array with the header row\nmy $aoa = [[\"id\", @keys]];\n\n# Process the data\nforeach my $item (@$data) {\n    my @row = ($item-&gt;{\"id\"});\n    foreach my $key (@keys) {\n        push @row, scalar @{$item-&gt;{$key} // []};\n    }\n    push @$aoa, \\@row;\n}\n\n# Write array of arrays as csv file\ncsv(in =&gt; $aoa, out =&gt; \"results.tsv\", sep_char =&gt; \"\\t\", eol =&gt; \"\\n\");\n</code></pre> See result <p>When you run this in, for example, this file, you'll obtain a text file in the following format:</p> id diseases exposures interventionsOrProcedures measures phenotypicFeatures treatments HG00096 0 0 1 3 0 0 HG00097 0 0 1 3 0 0 HG00099 0 0 1 3 0 0 HG00100 0 0 1 3 0 0 HG00101 0 0 1 3 0 0 HG00102 0 0 1 3 0 0 HG00103 1 0 1 3 0 0 HG00105 3 0 1 3 0 0 ... <p>Once you have the data in that format, you can process it however you prefer. Below, you'll find an example:</p> Example: Basic stats <pre><code>import pandas as pd\n\n# Load TSV file\ndf = pd.read_csv('results.tsv', sep='\\t')\n\n# Exclude the first column (assuming it's 'id')\ndf = df.iloc[:, 1:]\n\n# Initialize a dictionary to hold the statistics\nstats = {\n    'Statistic': ['Mean', 'Median', 'Max', 'Min', '25th Percentile', '75th Percentile', 'IQR', 'Standard Deviation']\n}\n\n# Calculate statistics for each column and add to the dictionary\nfor column in df.columns:\n    percentile_25 = df[column].quantile(0.25)\n    percentile_75 = df[column].quantile(0.75)\n\n    stats[column] = [\n        df[column].mean(),\n        df[column].median(),\n        df[column].max(),\n        df[column].min(),\n        percentile_25,\n        percentile_75,\n        percentile_75 - percentile_25,\n        df[column].std()\n    ]\n\n# Create a new DataFrame from the stats dictionary\nstats_df = pd.DataFrame(stats)\n\n# Save the statistics DataFrame to a CSV file\nstats_df.to_csv('column_statistics.csv', index=False)\n</code></pre> Statistic diseases exposures interventionsOrProcedures measures phenotypicFeatures treatments Mean 1.02 0.0 1.0 3.0 0.0 0.0 Median 1.0 0.0 1.0 3.0 0.0 0.0 Max 5.0 0.0 1.0 3.0 0.0 0.0 Min 0.0 0.0 1.0 3.0 0.0 0.0 25th Percentile 0.0 0.0 1.0 3.0 0.0 0.0 75th Percentile 2.0 0.0 1.0 3.0 0.0 0.0 IQR 2.0 0.0 0.0 0.0 0.0 0.0 Standard Deviation 0.92 0.0 0.0 0.0 0.0 0.0 <p>A similar approach but in <code>R</code>:</p> <pre><code># Load TSV file\ndf &lt;- read.csv(\"results.tsv\", sep = \"\\t\")\n\n# Exclude the first column (assuming it's 'id')\ndf &lt;- df[-1]\n\n# Calculate summary statistics for each numeric column\nsummary_stats &lt;- summary(df)\n\n# Save the summary statistics to a CSV file\nwrite.csv(summary_stats, file = 'column_statistics.csv')\n</code></pre> diseases exposures interventionsOrProcedures measures phenotypicFeatures treatments Min. :0.000 Min. :0 Min. :1 Min. :3 Min. :0 Min. :0 1st Qu.:0.000 1st Qu.:0 1st Qu.:1 1st Qu.:3 1st Qu.:0 1st Qu.:0 Median :1.000 Median :0 Median :1 Median :3 Median :0 Median :0 Mean   :1.023 Mean   :0 Mean   :1 Mean   :3 Mean   :0 Mean   :0 3rd Qu.:2.000 3rd Qu.:0 3rd Qu.:1 3rd Qu.:3 3rd Qu.:0 3rd Qu.:0 Max.   :5.000 Max.   :0 Max.   :1 Max.   :3 Max.   :0 Max.   :0 Example: Plots <p>For plotting, we recommend using one of Pheno-Ranker's utilities.</p> How can I compare all individuals in one or multiple cohorts? <p>We recommend using Pheno-Ranker in cohort mode.    </p> How can I match patients similar to mine in a cohort(s)? <p>We recommend using Pheno-Ranker in patient mode.</p> How can I create synthetic data in BFF or PXF data exchange formats?\" <p>We recommend using one of Pheno-Ranker's utilities.</p> How can I convert my BFF/PXF data into Machine Learning features? <p>We recommend using Pheno-Ranker that performs <code>one-hot</code> encoding while preserving the hierarchical relationships of the JSON data.</p>"},{"location":"faq/#last-change-2024-01-17-by-manuel-rueda","title":"last change 2024-01-17 by Manuel Rueda","text":""},{"location":"faq/#last-change-2024-01-17-by-manuel-rueda_1","title":"last change 2024-01-17 by Manuel Rueda","text":""},{"location":"faq/#last-change-2024-01-17-by-manuel-rueda_2","title":"last change 2024-01-17 by Manuel Rueda","text":""},{"location":"faq/#last-change-2024-01-17-by-manuel-rueda_3","title":"last change 2024-01-17 by Manuel Rueda","text":""},{"location":"faq/#last-change-2024-01-17-by-manuel-rueda_4","title":"last change 2024-01-17 by Manuel Rueda","text":""},{"location":"faq/#installation","title":"Installation","text":"I am installing <code>Convert-Pheno</code> from source (non-containerized version) but I can't make it work. Any suggestions?"},{"location":"faq/#problems-with-python-pyperler","title":"Problems with Python / PyPerler","text":"<p>About PyPerler installation</p> <p>Apart from PypPerler itself, you may need to install <code>cython3</code> and <code>libperl-dev</code> to make it work.</p> <p><code>sudo apt-get install cython3 libperl-dev</code></p>"},{"location":"faq/#last-change-2023-01-04-by-manuel-rueda_2","title":"last change 2023-01-04 by Manuel Rueda","text":""},{"location":"future-plans/","title":"\ud83d\udd2e Future Plans","text":"gantt     title Convert-Pheno Roadmap*     dateFormat  YYYY-MM-DD      section Relases     Alpha                     :done,      r1, 2023-01-01, 100d     Beta                      :active,    r2, after m1, 900d     v1                        :           r3, after r2, 200d      section Publication     Manuscript                :done,      m1, 2023-01-01, 110d     Submission                :done,      m2, after m1, 260d     Paper published           :milestone, after m2, 0d      section Input-Formats     CSV (in)        :done, f1, after f0,   350d      section Output-formats     CSV (out)       :done, f0, 2024-02-01, 70d     OMOP-CDM (out)  :crit,     f2, 2025-03-22, 200d      section Planned     OpenEHR (in)    :          f3, after r2,   200d  <p><code>*The roadmap is subject to revisions and may evolve over time</code></p>"},{"location":"future-plans/#last-change-2024-11-28-by-manuel-rueda","title":"last change 2024-11-28 by Manuel Rueda","text":""},{"location":"implementation/","title":"\ud83c\udfd7 Implementation","text":""},{"location":"implementation/#components","title":"Components","text":"<p><code>Convert-Pheno</code> is a versatile toolkit composed of multiple components. At its core is a Perl module that functions as a node for both the command-line interface and the API. The Perl module can be used in Python with the included Python Binding that works out-of-the-box with the containerized version. The Web App is built on top of the command-line interface.</p> %%{init: {'theme':'neutral'}}%% graph TB   subgraph \"Perl\"   A[Module]--&gt; B[CLI]   A[Module]--&gt; C[API]   end    subgraph \"Python / JavaScript\"   B --&gt; D[Web App UI]   end    subgraph \"Python\"   A --&gt; |Python Binding| E[Module]   E --&gt; F[API]   end     style A fill: #6495ED, stroke: #6495ED   style B fill: #6495ED, stroke: #6495ED   style C fill: #6495ED, stroke: #6495ED   style D fill: #AFEEEE, stroke: #AFEEEE   style E fill: #FFFF33, stroke: #FFFF33   style F fill: #FFFF33, stroke: #FFFF33  Diagram showing Convert-Pheno implementation <p>Which one should I use?</p> <p>Most users find the CLI suitable for their needs. Begin by experimenting with the data in the Web App UI Playground.</p> <p>Power users may want to check the module or the API version. </p>"},{"location":"implementation/#software-architecture","title":"Software architecture","text":"<p>The core module is divided into various sub-modules. The main package, <code>Convert::Pheno</code>, handles class initialization and employs the Moo module along with Types::Standard for data validation. After validation, the user-selected method (e.g., <code>pxf2bff</code>) is executed, directing the data to the respective independent modules, each tailored for converting a specific input format.</p> Why Perl? <p>The choice of Perl as a language is due to its inherent speed in text processing and its use of sigils to distinguish data types within intricate data structures.</p>"},{"location":"mapping-steps/","title":"\ud83d\udd0d Mapping Steps","text":""},{"location":"mapping-steps/#step-1-conversion-to-the-target-model","title":"Step 1: Conversion to the target model","text":"<p>Internally, all models are mapped to the Beacon v2 Models.</p> %%{init: {'theme':'neutral'}}%% graph LR   subgraph \"Step 1:Conversion to BFF\"   B[Phenopackets v2] --&gt;|pxf2bff| A   C[REDCap] --&gt;|redcap2bff| A[Beacon v2 Models]   D[OMOP-CDM] --&gt;|omop2bff| A   E[CDISC-ODM] --&gt;|cdisc2bff| A   G[CSV] --&gt;|csv2bff| A   end    subgraph \"Step 2:BFF to Final\"   A --&gt; |bff2pxf | F[Phenopackets v2]   A --&gt; |bff2omop| H[OMOP-CDM]   end    style A fill: #6495ED, stroke: #6495ED   style B fill: #FF7F50, stroke: #FF7F50   style C fill: #FF6965, stroke: #FF6965   style D fill: #3CB371, stroke: #3CB371   style E fill: #DDA0DD, stroke: #DDA0DD   style F fill: #FF7F50, stroke: #FF7F50   style G fill: #FFFF00, stroke: #FFFF00   style H fill: #3CB371, stroke: #3CB371   Convert-Pheno internal mapping steps Why use Beacon v2 as the target model? <ul> <li>JSON Schema Utilization: Beacon v2 employs JSON Schema for model content definition, facilitating transparency and accessibility in a collaborative environment compared to Phenopackets' Protobuf usage.</li> <li>Accommodation of Additional Properties: The Beacon v2 Models schema permits additional properties, enhancing adaptability and enabling near-lossless conversion, especially when using JSON in non-relational databases.</li> <li>Beacon v2 API Compatibility: The BFF is directly compatible with the Beacon v2 API ecosystem, a feature not available in Phenopackets without additional mapping.</li> <li>Expansion Possibility: Being based at CNAG, a genomics institution, the potential to extend Convert-Pheno's mapping to encompass other Beacon v2 entities was a significant consideration.</li> <li>Overlap with Phenopackets v2: Despite minor differences in nomenclature or hierarchy, many essential terms remain identical, encouraging interoperability.</li> </ul>"},{"location":"mapping-steps/#schema-mapping","title":"Schema mapping","text":"<p>When starting a new conversion between two data models, the first step is to map variables between the two data schemas. At the time of writting this (Sep-2023) the mapping of variables is still performed manually by human brains .</p> Mapping strategy: External or hardcoded? <p>In the early stages of development, we explored the possibility of employing configuration files to guide the mapping process as an alternative to hardcoded solutions. However, JSON data structures' complexity, mainly due to nesting, made this approach impractical for most scenarios, except for REDCap and CDISC-ODM data, which are mapped to Beacon v2 Models via configuration files.</p> <p>In the Mapping tables section (accessible via the 'Technical Details' tab on the left navigation bar), we outline the equivalencies between different schemas. These tables fulfill several purposes:</p> <ol> <li>It's a quick way to help out the Health Data community.</li> <li>Experts can check it out and suggest changes without digging into all the code.</li> <li>If you want to chip in and create a new conversion, you can start by making a mapping table.</li> </ol> <p>Notice</p> <p>Please note that accurately mapping, even between two standards, is a substantial undertaking. While we possess expertise in certain areas, we certainly don't claim mastery in all . We sincerely welcome any suggestions or feedback.</p> Contributing <p>While creating the code for a new format can be challenging, modifying properties in an existing one is much easier. Feel free to reach us should you plan to contribute.</p>"},{"location":"mapping-steps/#from-table-mappings-to-code","title":"From table mappings to code","text":"<p>These tables serve as a reference for implementing Convert-Pheno's source code. Each format conversion has a dedicated Perl submodule, and during implementation we verify that the converted output conforms to the final target data schema.</p>"},{"location":"mapping-steps/#lossless-or-lossy-conversion","title":"Lossless or lossy conversion?","text":"<p>When converting data from one data standard to another, it is important to consider the possibility of losing information due to differences in schema and field mapping. To mitigate this, we aimed for a lossless conversion by incorporating non-mappable variables as <code>additionalProperties</code> within the Beacon v2 Models schema. This allows users to access the original variables and their values through database queries, especially when using non-relational databases like MongoDB. </p> <p>During the conversion process, handling variables that cannot be directly mapped can result in one of two scenarios:</p> Unmappable variablesMatch to a different entity <p>Often, the input data model has variables that do not directly map to the target but are still useful to retain in the output format. If the target format allows for extra properties in a given term (as BFF does), these original variables are stored under the <code>_info</code> property (or <code>_</code> + \u2018property name\u2019). This commonly happens in conversions from OMOP CDM to BFF. </p> <p>Example extracted from <code>omop2bff</code> conversion:</p> See example <pre><code>\"interventionsOrProcedures\" : [\n       {\n          \"_info\" : {\n             \"PROCEDURE_OCCURRENCE\" : {\n                \"OMOP_columns\" : {\n                   \"modifier_concept_id\" : 0,\n                   \"modifier_source_value\" : null,\n                   \"person_id\" : 2,\n                   \"procedure_concept_id\" : 4163872,\n                   \"procedure_date\" : \"1955-10-22\",\n                   \"procedure_datetime\" : \"1955-10-22 00:00:00\",\n                   \"procedure_occurrence_id\" : 6,\n                   \"procedure_source_concept_id\" : 4163872,\n                   \"procedure_source_value\" : 399208008,\n                   \"procedure_type_concept_id\" : 38000275,\n                   \"provider_id\" : \"\\\\N\",\n                   \"quantity\" : \"\\\\N\", \n                   \"visit_detail_id\" : 0,\n                   \"visit_occurrence_id\" : 103\n                }\n             }\n          },\n          \"ageAtProcedure\" : {\n             \"age\" : {\n                \"iso8601duration\" : \"35Y\"\n             }\n          },\n          \"dateOfProcedure\" : \"1955-10-22\",\n          \"procedureCode\" : {\n             \"id\" : \"SNOMED:399208008\",\n             \"label\" : \"Plain chest X-ray\"\n          }\n       }\n ]\n</code></pre> <p>Example extracted from <code>redcap2bff</code> conversion:</p> See example <pre><code>\"treatments\" : [\n       {\n          \"_info\" : {\n             \"dose\" : null,\n             \"drug\" : \"budesonide\",\n             \"drug_name\" : \"budesonide\",\n             \"duration\" : null,\n             \"field\" : \"budesonide_oral_status\",\n             \"route\" : \"oral\",\n             \"start\" : null,\n             \"status\" : \"never treated\",\n             \"value\" : 1\n          },\n          \"doseIntervals\" : [],\n          \"routeOfAdministration\" : {\n             \"id\" : \"NCIT:C38288\",\n             \"label\" : \"Oral Route of Administration\"\n          },\n          \"treatmentCode\" : {\n             \"id\" : \"NCIT:C1027\",\n             \"label\" : \"Budesonide\"\n          }\n       }\n]\n</code></pre> <p>Example of longitudinal data stored under <code>_visit</code> in a <code>omop2bff</code> conversion:</p> See example <pre><code>\"_visit\" : {\n        \"_info\" : {\n           \"VISIT_OCCURRENCE\" : {\n              \"OMOP_columns\" : {\n                 \"admitting_source_concept_id\" : 0,\n                 \"admitting_source_value\" : null,\n                 \"care_site_id\" : \"\\\\N\",\n                 \"discharge_to_concept_id\" : 0,\n                 \"discharge_to_source_value\" : null,\n                 \"person_id\" : 3,\n                 \"preceding_visit_occurrence_id\" : 347,\n                 \"provider_id\" : \"\\\\N\",\n                 \"visit_concept_id\" : 9201,\n                 \"visit_end_date\" : \"1972-12-21\",\n                 \"visit_end_datetime\" : \"1972-12-21 00:00:00\",\n                 \"visit_occurrence_id\" : 312,\n                 \"visit_source_concept_id\" : 0,\n                 \"visit_source_value\" : \"5d035dd1-30d9-4389-b94c-64947bf1f18c\",\n                 \"visit_start_date\" : \"1972-12-20\",\n                 \"visit_start_datetime\" : \"1972-12-20 00:00:00\",\n                 \"visit_type_concept_id\" : 44818517\n              }\n           }\n        },\n        \"concept\" : {\n           \"id\" : \"Visit:IP\",\n           \"label\" : \"Inpatient Visit\"\n        },\n        \"end_date\" : \"1972-12-21T00:00:00Z\",\n        \"id\" : \"312\",\n        \"occurrence_id\" : 312,\n        \"start_date\" : \"1972-12-20T00:00:00Z\",\n        \"type\" : {\n           \"id\" : \"Visit Type:OMOP4822465\",\n           \"label\" : \"Visit derived from encounter on claim\"\n        }\n     },\n     \"featureType\" : {\n        \"id\" : \"SNOMED:428251008\",\n        \"label\" : \"History of appendectomy\"\n     },\n     \"onset\" : {\n        \"iso8601duration\" : \"56Y\"\n     }\n}\n</code></pre> <p>When a variable corresponds to other entities in Beacon v2 Models, it is stored within the <code>info</code> term of the individuals entity. For instance, a <code>PXF</code> file may contain the biosamples property, which doesn't find a direct match in the individuals entity as it corresponds to the biosamples entity in Beacon v2 Models. To ensure the retention of this information, we place it under <code>info.phenopacket.biosamples</code>.</p> <p>Example extracted from <code>pxf2bff</code> conversion:</p> See example <pre><code>\"info\" : {\n          \"phenopacket\" : {\n             \"biosamples\" : [\n                {\n                   \"id\" : \"biosample.1\",\n                   \"phenotypicFeatures\" : [\n                      {\n                         \"excluded\" : false,\n                         \"type\" : {\n                            \"id\" : \"HP:0003798\",\n                            \"label\" : \"Nemaline bodies\"\n                         }\n                      }\n                   ],\n                   \"procedure\" : {\n                      \"bodySite\" : {\n                         \"id\" : \"UBERON:0002378\",\n                         \"label\" : \"muscle of abdomen\"\n                      },\n                      \"code\" : {\n                         \"id\" : \"NCIT:C51895\",\n                         \"label\" : \"Muscle Biopsy\"\n                      },\n                      \"performed\" : {\n                         \"age\" : {\n                            \"iso8601duration\" : \"P1D\"\n                         }\n                      }\n                   },\n                   \"sampledTissue\" : {\n                      \"id\" : \"UBERON:0002378\",\n                      \"label\" : \"muscle of abdomen\"\n                   }\n                }\n             ]\n       }\n}\n</code></pre>"},{"location":"mapping-steps/#preservation-and-augmentation-of-ontologies","title":"Preservation and augmentation of ontologies","text":"<p>One of the advantages of Beacon/Phenopackets v2 is that they do not prescribe the use of specific ontologies, thus allowing us to retain the original ontologies, except to fill in missing terms in required fields.</p> Which ontologies/terminologies are supported? <p>If the input files contain ontology tems, the ontologies will be preserved and remain intact after the conversion process, except for:</p> <ul> <li>Beacon v2 Models and Phenopackets v2: the property <code>sex</code> is converted to NCI Thesaurus via database search.</li> <li>OMOP CDM: the properties <code>sex</code>, <code>ethnicity</code>, and <code>geographicOrigin</code> are converted to NCI Thesaurus via database search.</li> </ul> CSV REDCap CDISC-ODM OMOP-CDM Phenopackets v2 Beacon v2 Models Data mapping \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Add ontologies \u2713 \u2713 \u2713 <code>--ohdsi-db</code> <p>Database Search Feature</p> <p>For input types that do not contain ontologies, such as <code>CSV</code>, REDCap, and CDISC-ODM, we perform a database search to fetch ontologies from a variety of trusted databases. Supported databases include:</p> <ul> <li>Athena-OHDSI standardized vocabulary, which includes multiple terminologies, such as SNOMED, RxNorm or LOINC</li> <li>NCI Thesaurus</li> <li>ICD-10 terminology</li> <li>CDISC (Study Data Tabulation Model Terminology)</li> <li>OMIM Online Mendelian Inheritance in Man</li> <li>HPO Human Phenotype Ontology (Note that prefixes are <code>HP:</code>, without the <code>O</code>)</li> </ul> About text similarity in database searches <p><code>Convert-Pheno</code> comes with several pre-configured ontology/terminology databases. It supports three types of label-based search strategies:</p> Composite Similarity Score <p>The composite similarity score is computed as a weighted sum of two measures: the token-based similarity and the normalized Levenshtein similarity.</p>"},{"location":"mapping-steps/#1-exact-default","title":"1. <code>exact</code> (default)","text":"<p>Returns only exact matches for the given label string. If the label is not found exactly, no results are returned.</p>"},{"location":"mapping-steps/#2-mixed-use-search-mixed","title":"2. <code>mixed</code> (use <code>--search mixed</code>)","text":"<p>Hybrid search: First tries to find an exact label match. If none is found, it performs a token-based similarity search and returns the closest matching concept based on the highest similarity score.</p>"},{"location":"mapping-steps/#3-fuzzy-use-search-fuzzy","title":"3. \u2728 <code>fuzzy</code> (use <code>--search fuzzy</code>)","text":"<p>Hybrid search with fuzzy ranking: Like <code>mixed</code>, it starts with an exact match attempt. If that fails, it performs a weighted similarity search, where: - 90% of the score comes from token-based similarity (e.g., cosine or Dice coefficient), - 10% comes from the normalized Levenshtein similarity.</p> <p>The concept with the highest composite score is returned.</p> <p>Note: The normalized Levenshtein similarity is computed on top of the candidate results produced by the full text search. In this approach, an initial full text search (using token-based methods) returns a set of potential matches. The fuzzy search then refines these results by applying the normalized Levenshtein distance to better handle minor typographical differences, ensuring that the final composite score reflects both overall token similarity and fine-grained character-level differences.</p>"},{"location":"mapping-steps/#example-search-behavior","title":"\ud83d\udd0d Example Search Behavior","text":"<p>Query: <code>Exercise pain management</code> - With <code>--search exact</code>: \u2705 Match found \u2014 Exercise Pain Management</p> <p>Query: <code>Brain Hemorrhage</code> - With <code>--search mixed</code>:   - \u274c No exact match   - \u2705 Closest match by similarity: Intraventricular Brain Hemorrhage</p>"},{"location":"mapping-steps/#similarity-threshold","title":"\ud83d\udca1 Similarity Threshold","text":"<p>The <code>--min-text-similarity-score</code> option sets the minimum threshold for <code>mixed</code> and <code>fuzzy</code> searches. - Default: <code>0.8</code> (conservative) - Lowering the threshold may increase recall but may introduce irrelevant matches.</p>"},{"location":"mapping-steps/#performance-note","title":"\u26a0\ufe0f Performance Note","text":"<p>Both <code>mixed</code> and <code>fuzzy</code> modes are more computationally intensive and can produce unexpected or less interpretable matches. Use them with care, especially on large datasets.</p>"},{"location":"mapping-steps/#example-results-table","title":"\ud83e\uddea Example Results Table","text":"<p>Below is an example showing how the query <code>Sudden Death Syndrome</code> performs using different search modes against the NCIt ontology:</p> Query Search NCIt match (label) NCIt code Cosine Dice Levenshtein (Normalized) Composite Sudden Death Syndrome exact NA NA NA NA NA NA mixed CDISC SDTM Sudden Death Syndrome Type Terminology NCIT:C101852 0.65 0.60 NA NA Family History of Sudden Arrythmia Death Syndrome NCIT:C168019 0.65 0.60 NA NA Family History of Sudden Infant Death Syndrome NCIT:C168209 0.65 0.60 NA NA Sudden Infant Death Syndrome NCIT:C85173 0.86 0.86 NA NA \u2728 fuzzy CDISC SDTM Sudden Death Syndrome Type Terminology NCIT:C101852 0.65 0.60 0.43 0.63 Family History of Sudden Arrythmia Death Syndrome NCIT:C168019 0.65 0.60 0.43 0.63 Family History of Sudden Infant Death Syndrome NCIT:C168209 0.65 0.60 0.46 0.63 Sudden Infant Death Syndrome NCIT:C85173 0.86 0.86 0.75 0.85 <p>Interpretation: </p> <ul> <li> <p>With <code>exact</code>, there are no matches.</p> </li> <li> <p>With <code>mixed</code>, the best match will be <code>Sudden Infant Death Syndrome</code>.</p> </li> <li> <p>With <code>fuzzy</code>, the composite score (90% token-based + 10% Levenshtein similarity) is used to rank results.   The highest match is <code>Sudden Infant Death Syndrome</code>, with a composite score of 0.85.</p> </li> </ul> <p>\u2728 Now we introduce a typo on the query <code>Sudden Infant Deth Syndrome</code>:</p> Query Mode Candidate Label Code Cosine Dice Levenshtein (Normalized) Composite Sudden Infant Deth Syndrome fuzzy CDISC SDTM Sudden Death Syndrome Type Terminology NCIT:C101852 0.38 0.36 0.33 0.37 Family History of Sudden Arrythmia Death Syndrome NCIT:C168019 0.38 0.36 0.43 0.38 Family History of Sudden Infant Death Syndrome NCIT:C168209 0.57 0.55 0.59 0.57 Sudden Infant Death Syndrome NCIT:C85173 0.75 0.75 0.96 0.77 <p>To capture the best match we would need to lower the threshold to  <code>--min-text-similarity-score 0.75</code></p> <p>It is possible to change the weight of Levenshtein similarity via <code>--levenshtein-weight &lt;floating 0.0 - 1.0&gt;</code>.</p>"},{"location":"mapping-steps/#1-token-based-similarity","title":"1. Token-Based Similarity","text":"<p>This is calculated using methods like cosine or Dice similarity to measure how similar the tokens (words) of two strings are.</p>"},{"location":"mapping-steps/#2-normalized-levenshtein-similarity","title":"2. Normalized Levenshtein Similarity","text":"<p>The normalized Levenshtein similarity is defined as:</p> \\[ \\text{NormalizedLevenshtein}(s_1, s_2) = 1 - \\frac{\\text{lev}(s_1, s_2)}{\\max(|s_1|, |s_2|)} \\] <p>Where: - \\(\\text{lev}(s_1, s_2)\\) is the Levenshtein edit distance\u2014the minimum number of insertions, deletions, or substitutions required to change \\(s_1\\) into \\(s_2\\). - \\(|s_1|\\) and \\(|s_2|\\) are the lengths of the strings \\(s_1\\) and \\(s_2\\), respectively.</p> <p>This formula produces a score between 0 and 1, with 1.0 meaning identical strings and 0.0 meaning completely different strings.</p>"},{"location":"mapping-steps/#3-composite-score-formula","title":"3. Composite Score Formula","text":"<p>The final composite similarity score \\(C\\) is a weighted combination of the two metrics:</p> \\[ C(s_1, s_2) = \\alpha \\cdot \\text{TokenSimilarity}(s_1, s_2) + \\beta \\cdot \\text{NormalizedLevenshtein}(s_1, s_2) \\] <p>Where: - \\(\\alpha\\) (or <code>token_weight</code>) is the weight assigned to the token-based similarity. - \\(\\beta\\) (or <code>lev_weight</code>) is the weight assigned to the normalized Levenshtein similarity.</p> <p>A common default is to set \\(\\alpha = 0.9\\) and \\(\\beta = 0.1\\), emphasizing the token-based similarity. However, for short strings (4\u20135 words), you might consider adjusting the balance (for example, \\(\\alpha = 0.95\\) and \\(\\beta = 0.05\\)) if small typographical differences are less critical.</p>"},{"location":"mapping-steps/#step-2-conversion-to-the-final-model","title":"Step 2: Conversion to the final model","text":"<p>Data validation</p> <p>To ensure the integrity and validity of converted outputs, we employ external validation tools during development and in unit tests. Specifically, we used the bff-tools validate for Beacon Friendly Format (BFF) and phenopacket-tools for Phenotype Exchange Format (PXF). These validators were instrumental in ensuring converted data adhere to the respective schemas and standards; for example, conversions were validated until the output was 100% compliant with the target schema. The same validation process is applied to Beacon v2 and OMOP CDM outputs. By preserving non-mapped variables where appropriate and applying rigorous validation, we aim to mitigate information loss and maximise fidelity of the converted data.</p> <p>Important: Convert-Pheno does not validate your input data. Input validation is out of scope for the software. If fields are missing or malformed, Convert-Pheno will handle these cases internally and apply default values where appropriate, but it will not verify that your source files are complete or correct. We therefore recommend validating and cleaning source files before conversion.</p> <p>See:</p> <ul> <li>bff-tools validate</li> <li>phenopacket-tools</li> <li>OMOP CSV Validator</li> </ul>"},{"location":"mapping-steps/#to-phenopackets","title":"To Phenopackets","text":"<p>If the output is set to Phenopackets v2 then a second step (<code>bff2pxf</code>) is performed (see diagram above).</p> BFF and PXF community alignment <p>At present, we have prioritized mapping the terms that we deem most critical in facilitating basic semantic interoperability. We anticipate that Beacon v2 Models will become more aligned with Phenopackets v2, which will simplify the conversion process in future updates. We aim to refine the mappings in future iterations, with the community providing a wider range of case studies.</p>"},{"location":"mapping-steps/#to-omop-cdm","title":"To OMOP CDM","text":"<p>If the output is set to OMOP CDM then a second step (<code>bff2omop</code>) is performed (see diagram above).</p>"},{"location":"omop-cdm/","title":"\ud83c\udfe5 OMOP\u2011CDM","text":"<p>OMOP CDM stands for Observational Medical Outcomes Partnership Common Data Model. OMOP CDM documentation.</p> <p> </p> Image extracted from www.ohdsi.org <p>The OMOP CDM is designed to be database-agnostic, which means it can be implemented using different relational database management systems, with PostgreSQL being a popular choice.</p> <p><code>Convert-Pheno</code> is capable of performing both file-based conversions (from PostgreSQL exports in <code>.sql</code> or from any other SQL database via <code>.csv</code> files) and real-time conversions (e.g., from SQL queries) as long as the data has been converted to the accepted JSON format.</p> About OMOP CDM longitudinal data <p>OMOP CDM stores <code>visit_occurrence_id</code> for each <code>person_id</code> in the <code>VISIT_OCCURRENCE table</code>. However, Beacon v2 Models currently lack a way to store longitudinal data. To address this, we added a property named <code>_visit</code> to each record, which stores visit information. This property will be serialized only if the <code>VISIT_OCCURRENCE</code> table is provided.</p> <p>OMOP CDM supported version(s)</p> <p>Currently, Convert-Pheno supports versions 5.3 and 5.4 of OMOP CDM, and its prepared to support v6 once we can test the code with v6 projects.</p>"},{"location":"omop-cdm/#omop-as-input","title":"OMOP as input","text":"Command-lineModuleAPI <p>When using the <code>convert-pheno</code> command-line interface, simply ensure the correct syntax is provided.</p> Does <code>Convert-Pheno</code> accept <code>gz</code> files? <p>Yes, both input and output files can be gzipped to save space. However, it's important to note that the gzip layer introduces an overhead. </p> <p>This overhead can be substantial, potentially doubling the processing time in <code>--stream</code> mode when handling PostgreSQL dumps as input.</p> About <code>--max-lines-sql</code> default value <p>Please note that for PostgreSQL dumps, we have configured <code>--max-lines-sql=500</code> which is suitable for testing purposes. However, for real data, it is recommended to increase this limit to match the size of your largest table. This flag does not apply when your input files are <code>CSV</code>.</p> Small to medium-sized files (&lt;1M rows)Large files (&gt;1M rows) <p>For large files, <code>Convert-Pheno</code> allows for a different approach. The files can be parsed incrementally (i.e., line-by-line).</p> <p>To choose incremental data processing we'll be using the flag <code>--stream</code>:</p> <code>--stream</code> mode supported output <p>We only support output to BFF (<code>-obff</code>).</p> <p>For developers who wish to retrieve data in real-time, we also offer the option of using the module version. With this option, the developer has to handle database credentials, queries, etc. using one of the many available PostgreSQL drivers.</p> <p>The idea is to pass the essential information to <code>Convert-Pheno</code> as a hash (in Perl) or dictionary (in Python). It is not necessary to send all the tables shown in the example, only the ones you wish to transform.</p> <p>Tip</p> <p>The defintions are stored in table <code>CONCEPT</code>. If you send the complete <code>CONCEPT</code> table then <code>Convert-Pheno</code> will be able to find a match, otherwise it will require setting the parameter <code>ohdsi_db = 1</code> (true).</p> PerlPython <pre><code>my $data = \n{\n  method =&gt; 'omop2bff',\n  ohdsi_db =&gt; 0,\n  data =&gt; \n  {\n      'CONCEPT' =&gt; [\n                     {\n                       'concept_class_id' =&gt; 'Undefined',\n                       'concept_code' =&gt; 'No matching concept',\n                       'concept_id' =&gt; 0,\n                       'concept_name' =&gt; 'No matching concept',\n                       'domain_id' =&gt; 'Metadata',\n                       'invalid_reason' =&gt; undef,\n                       'standard_concept' =&gt; undef,\n                       'valid_end_date' =&gt; '2099-12-31',\n                       'valid_start_date' =&gt; '1970-01-01',\n                       'vocabulary_id' =&gt; 'None'\n                     },\n                     {\n                       'concept_class_id' =&gt; 'Gender',\n                       'concept_code' =&gt; 'F',\n                       'concept_id' =&gt; 8532,\n                       'concept_name' =&gt; 'FEMALE',\n                       'domain_id' =&gt; 'Gender',\n                       'invalid_reason' =&gt; undef,\n                       'standard_concept' =&gt; 'S',\n                       'valid_end_date' =&gt; '2099-12-31',\n                       'valid_start_date' =&gt; '1970-01-01',\n                       'vocabulary_id' =&gt; 'Gender'\n                     },\n                     {\n                       'concept_class_id' =&gt; 'Clinical Observation',\n                       'concept_code' =&gt; '8331-1',\n                       'concept_id' =&gt; 3006322,\n                       'concept_name' =&gt; 'Oral temperature',\n                       'domain_id' =&gt; 'Measurement',\n                       'invalid_reason' =&gt; undef,\n                       'standard_concept' =&gt; 'S',\n                       'valid_end_date' =&gt; '2099-12-31',\n                       'valid_start_date' =&gt; '1996-09-06',\n                       'vocabulary_id' =&gt; 'LOINC'\n                     }\n                   ],\n      'MEASUREMENT' =&gt; [\n                         {\n                           'measurement_concept_id' =&gt; 3006322,\n                           'measurement_date' =&gt; '1998-10-03',\n                           'measurement_datetime' =&gt; '1998-10-03 00:00:00',\n                           'measurement_id' =&gt; 10204,\n                           'measurement_source_concept_id' =&gt; 3006322,\n                           'measurement_source_value' =&gt; '8331-1',\n                           'measurement_time' =&gt; '1998-10-03',\n                           'measurement_type_concept_id' =&gt; 5001,\n                           'operator_concept_id' =&gt; 0,\n                           'person_id' =&gt; 974,\n                           'provider_id' =&gt; 0,\n                           'range_high' =&gt; '\\\\N',\n                           'range_low' =&gt; '\\\\N',\n                           'unit_concept_id' =&gt; 0,\n                           'unit_source_value' =&gt; undef,\n                           'value_as_concept_id' =&gt; 0,\n                           'value_as_number' =&gt; 4,\n                           'value_source_value' =&gt; undef,\n                           'visit_detail_id' =&gt; 0,\n                           'visit_occurrence_id' =&gt; 64994\n                         }\n                       ],\n      'PERSON' =&gt; [\n                    {\n                      'birth_datetime' =&gt; '1963-12-31 00:00:00',\n                      'care_site_id' =&gt; '\\\\N',\n                      'day_of_birth' =&gt; 31,\n                      'ethnicity_concept_id' =&gt; 0,\n                      'ethnicity_source_concept_id' =&gt; 0,\n                      'ethnicity_source_value' =&gt; 'west_indian',\n                      'gender_concept_id' =&gt; 8532,\n                      'gender_source_concept_id' =&gt; 0,\n                      'gender_source_value' =&gt; 'F',\n                      'location_id' =&gt; '\\\\N',\n                      'month_of_birth' =&gt; 12,\n                      'person_id' =&gt; 974,\n                      'person_source_value' =&gt; '001f4a87-70d0-435c-a4b9-1425f6928d33',\n                      'provider_id' =&gt; '\\\\N',\n                      'race_concept_id' =&gt; 8516,\n                      'race_source_concept_id' =&gt; 0,\n                      'race_source_value' =&gt; 'black',\n                      'year_of_birth' =&gt; 1963\n                    }\n                  ]\n       }\n};\n</code></pre> <pre><code>data = \n{\n  \"method\": \"omop2bff\",\n  \"ohdsi_db\": False,\n  \"data\": {\n    \"CONCEPT\": [\n      {\n        \"concept_class_id\": \"Undefined\",\n        \"concept_code\": \"No matching concept\",\n        \"concept_id\": 0,\n        \"concept_name\": \"No matching concept\",\n        \"domain_id\": \"Metadata\",\n        \"invalid_reason\": null,\n        \"standard_concept\": null,\n        \"valid_end_date\": \"2099-12-31\",\n        \"valid_start_date\": \"1970-01-01\",\n        \"vocabulary_id\": \"None\"\n      },\n      {\n        \"concept_class_id\": \"Gender\",\n        \"concept_code\": \"F\",\n        \"concept_id\": 8532,\n        \"concept_name\": \"FEMALE\",\n        \"domain_id\": \"Gender\",\n        \"invalid_reason\": null,\n        \"standard_concept\": \"S\",\n        \"valid_end_date\": \"2099-12-31\",\n        \"valid_start_date\": \"1970-01-01\",\n        \"vocabulary_id\": \"Gender\"\n      },\n      {\n        \"concept_class_id\": \"Clinical Observation\",\n        \"concept_code\": \"8331-1\",\n        \"concept_id\": 3006322,\n        \"concept_name\": \"Oral temperature\",\n        \"domain_id\": \"Measurement\",\n        \"invalid_reason\": null,\n        \"standard_concept\": \"S\",\n        \"valid_end_date\": \"2099-12-31\",\n        \"valid_start_date\": \"1996-09-06\",\n        \"vocabulary_id\": \"LOINC\"\n      }\n    ],\n    \"MEASUREMENT\": [\n      {\n        \"measurement_concept_id\": 3006322,\n        \"measurement_date\": \"1998-10-03\",\n        \"measurement_datetime\": \"1998-10-03 00:00:00\",\n        \"measurement_id\": 10204,\n        \"measurement_source_concept_id\": 3006322,\n        \"measurement_source_value\": \"8331-1\",\n        \"measurement_time\": \"1998-10-03\",\n        \"measurement_type_concept_id\": 5001,\n        \"operator_concept_id\": 0,\n        \"person_id\": 974,\n        \"provider_id\": 0,\n        \"range_high\": \"\\\\N\",\n        \"range_low\": \"\\\\N\",\n        \"unit_concept_id\": 0,\n        \"unit_source_value\": null,\n        \"value_as_concept_id\": 0,\n        \"value_as_number\": 4,\n        \"value_source_value\": null,\n        \"visit_detail_id\": 0,\n        \"visit_occurrence_id\": 64994\n      }\n    ],\n    \"PERSON\": [\n      {\n        \"birth_datetime\": \"1963-12-31 00:00:00\",\n        \"care_site_id\": \"\\\\N\",\n        \"day_of_birth\": 31,\n        \"ethnicity_concept_id\": 0,\n        \"ethnicity_source_concept_id\": 0,\n        \"ethnicity_source_value\": \"west_indian\",\n        \"gender_concept_id\": 8532,\n        \"gender_source_concept_id\": 0,\n        \"gender_source_value\": \"F\",\n        \"location_id\": \"\\\\N\",\n        \"month_of_birth\": 12,\n        \"person_id\": 974,\n        \"person_source_value\": \"001f4a87-70d0-435c-a4b9-1425f6928d33\",\n        \"provider_id\": \"\\\\N\",\n        \"race_concept_id\": 8516,\n        \"race_source_concept_id\": 0,\n        \"race_source_value\": \"black\",\n        \"year_of_birth\": 1963\n      }\n    ]\n  }\n}\n</code></pre> <p>All said for the Module also works for the API. See example data here.</p> <pre><code>{\n  \"data\": { ... },\n  \"method\": \"omop2bff\",\n  \"ohdsi_db\": true\n}\n</code></pre>"},{"location":"omop-cdm/#all-tables-at-once","title":"All tables at once","text":"<p>Usage:</p> <p><pre><code>convert-pheno -iomop omop_dump.sql -obff individuals.json\n</code></pre> or when gzipped... <pre><code>convert-pheno -iomop omop_dump.sql.gz -obff individuals.json.gz\n</code></pre> with multiple CSVs (one CSV per table)... <pre><code>convert-pheno -iomop *csv -obff individuals.json.gz\n</code></pre></p>"},{"location":"omop-cdm/#selected-tables","title":"Selected table(s)","text":"<p>It is possible to convert selected tables. For instance, in case you only want to convert <code>DRUG_EXPOSURE</code> table use the option <code>--omop-tables</code>. The option accepts a list of tables (case insensitive) separated by spaces:</p> <p>About tables <code>CONCEPT</code> and <code>PERSON</code></p> <p>Tables <code>CONCEPT</code> and <code>PERSON</code> are always loaded as they're needed for the conversion. You don't need to specify them.</p> <pre><code>convert-pheno -iomop omop_dump.sql -obff individuals.json --omop-tables DRUG_EXPOSURE\n</code></pre> <p>Using this approach you will be able to submit multiple jobs in parallel.</p> What if my <code>CONCEPT</code> table does not contain all standard <code>concept_id</code>(s) <p>In this case, you can use the flag <code>--ohdsi-db</code> that will enable checking an internal database whenever the <code>concept_id</code> can not be found inside your <code>CONCEPT</code> table.</p> <pre><code>convert-pheno -iomop omop_dump.sql -obff individuals_measurement.json --omop-tables DRUG_EXPOSURE --ohdsi-db\n</code></pre> RAM memory usage in <code>--no-stream</code> mode (default) <p>When working with <code>-iomop</code> and <code>--no-stream</code>, <code>Convert-Pheno</code> will consolidate all the values corresponding to a given attribute <code>person_id</code> under the same object. In order to do this, we need to store all data in the RAM memory. The reason for storing the data in RAM is because the rows are not adjacent (they are not pre-sorted by <code>person_id</code>) and can originate from distinct tables.</p> Number of rows Estimated RAM memory Estimated time 100K &lt;1GB 5s 500K 1GB 15s 1M 2GB 30s 2M 4GB 1m <p>1 x Intel(R) Xeon(R) W-1350P @ 4.00GHz - 32GB RAM - SSD</p> <p>If your computer only has 4GB-8GB of RAM and you plan to convert large files we recommend you to use the flag <code>--stream</code> which will process your tables incrementally (i.e.,line-by-line), instead of loading them into memory. </p>"},{"location":"omop-cdm/#all-tables-at-once_1","title":"All tables at once","text":"<pre><code>convert-pheno -iomop omop_dump.sql.gz -obff individuals.json.gz --stream\n</code></pre> <p>About OMOP core tables and RAM usage</p> <p>Tables <code>CONCEPT</code> and <code>PERSON</code> are always loaded in RAM.</p> <p><code>VISIT_OCCURRENCE</code> will also be loaded if present, and this can consume a lot of RAM depending on its size. You might simply skip this table when exporting OMOP CDM data, as its information is only used as additional property <code>_visit</code>, but it is not part of the Beacon v2 or Phenopackets schema.</p>"},{"location":"omop-cdm/#selected-tables_1","title":"Selected table(s)","text":"<p>You can narrow down the selection to a set of table(s).</p> About tables <code>CONCEPT</code> and <code>PERSON</code> <p>Tables <code>CONCEPT</code> and <code>PERSON</code> are always loaded as they're needed for the conversion. You don't need to specify them.</p> <pre><code>convert-pheno -iomop omop_dump.sql.gz -obff individuals_measurement.json.gz --omop-tables DRUG_EXPOSURE --stream\n</code></pre> <p>Running multiple jobs in <code>--stream</code> mode will create a bunch of <code>JSON</code> files instead of one. It's OK, as the files we're creating are intermediate files.</p> Pros and Cons of incremental data load (<code>--stream</code> mode) <p>Incremental data load facilitates the processing of huge files. The only substantive difference compared to the <code>--no-stream</code> mode is that the data will not be consolidated at the patient or individual level, which is merely a cosmetic concern. Ultimately, the data will be loaded into a database, such as MongoDB, where the linking of data through keys can be managed. In most cases, the implementation of a pre-built API, such as the one described in the B2RI documentation, will be added to further enhance the functionality.</p> Number of rows Estimated RAM memory Estimated time 100K 500MB 7s 500K 500MB 18s 1M 500MB 35s 2M 500MB 1m5s <p>1 x Intel(R) Xeon(R) W-1350P @ 4.00GHz - 32GB RAM - SSD</p> <p>Note that the output JSON files generated in <code>--stream</code> mode will always include information from the <code>PERSON</code> and <code>CONCEPT</code> tables. Therefore, both tables must be loaded into RAM (along with <code>VISIT_OCCURRENCE</code> if present). The size of these tables will obviously impact RAM usage. Although having this information is not a mandatory requirement for MongoDB, it helps in validating the data against Beacon v2 JSON schemas. According to JSON Schema terminology, these files contain <code>required</code> properties for BFF and PXF. For more details on validation, refer to the BFF Validator.</p> About parallelization and speed <p><code>Convert-Pheno</code> has been optimized for speed, and, in general the CLI results are generated almost immediatly. For instance, all tests with synthetic data take less than a second or a few seconds to complete. It should be noted that the speed of the results depends on the performance of the CPU and disk speed. When <code>Convert-Pheno</code> has to retrieve ontologies from a database to annotate the data, the processing takes longer.</p> <p>The calculation is I/O limited and using internal threads did not speed up the calculation. Another valid option is to run simultaneous jobs with external tools such as GNU Parallel, but keep in mind that SQLite database may complain.</p> <p>As a final consideration, it is important to remember that pheno-clinical data conversions are executed only \"once\". The goal is obtaining intermediate files which will be later loaded into a database. If a large file has been converted, it is verly likely that the performance bottleneck will not occur at the <code>Convert-Pheno</code> step, but rather during the database load.</p>"},{"location":"omop2bff/","title":"\ud83d\udd04 OMOP to BFF","text":"<p>OMOP to BFF - Schemas</p> <ul> <li>OMOP CDM v5.4 tables</li> <li>Beacon v2 Models - individuals</li> </ul> <p>Information</p> <p>The Beacon v2 schema enforces the presence of specific properties to achieve successful validation. In cases where no suitable match is found, DEFAULT values are employed to guarantee conformity.</p>"},{"location":"omop2bff/#version-025","title":"Version 0.25","text":""},{"location":"omop2bff/#terms","title":"Terms","text":""},{"location":"omop2bff/#diseases","title":"diseases","text":"OMOP Table(s) OMOP Variable BFF JSON path CONDITION_OCCURRENCE, PERSON condition_start_date, birth_datetime diseases.ageOfOnset CONDITION_OCCURRENCE condition_concept_id diseases.diseaseCode CONDITION_OCCURRENCE All variables diseases._info CONDITION_OCCURRENCE condition_status_concept_id diseases.stage CONDITION_OCCURRENCE person_id, visit_occurrence_id diseases._visit"},{"location":"omop2bff/#ethnicity","title":"ethnicity","text":"OMOP Table(s) OMOP Variable BFF JSON path PERSON race_source_value ethnicity"},{"location":"omop2bff/#exposures","title":"exposures","text":"OMOP Table(s) OMOP Variable BFF JSON path OBSERVATION, PERSON observation_date, birth_datetime exposures.ageAtExposure OBSERVATION observation_date exposures.date DEFAULT exposures.duration OBSERVATION All variables exposures._info OBSERVATION observation_concept_id exposures.exposureCode OBSERVATION unit_concept_id exposures.unit OBSERVATION value_as_number exposures.value"},{"location":"omop2bff/#geographicorigin","title":"geographicOrigin","text":"OMOP Table(s) OMOP Variable BFF JSON path PERSON ethnicity_source_value geographicOrigin"},{"location":"omop2bff/#id","title":"id","text":"OMOP Table(s) OMOP Variable BFF JSON path PERSON person_id id"},{"location":"omop2bff/#info","title":"info","text":"OMOP Table(s) OMOP Variable BFF JSON path PERSON All variables info.PERSON PERSON birth_datetime info.dateOfBirth"},{"location":"omop2bff/#interventionsorprocedures","title":"interventionsOrProcedures","text":"OMOP Table(s) OMOP Variable BFF JSON path PROCEDURE_OCCURRENCE, PERSON procedure_date, birth_datetime interventionsOrProcedures.ageAtProcedure PROCEDURE_OCCURRENCE procedure_date interventionsOrProcedures.dateOfProcedure PROCEDURE_OCCURRENCE All variables interventionsOrProcedures._info PROCEDURE_OCCURRENCE procedure_concept_id interventionsOrProcedures.procedureCode"},{"location":"omop2bff/#karyotypicsex","title":"karyotypicSex","text":"<p>NA</p>"},{"location":"omop2bff/#measures","title":"measures","text":"OMOP Table(s) OMOP Variable BFF JSON path MEASUREMENT measurement_concept_id measures.assayCode MEASUREMENT measurement_date measures.date MEASUREMENT value_as_concept_id measures.measurementValue MEASUREMENT unit_concept_id measures.measurementValue.quantity.unit MEASUREMENT value_as_number measures.measurementValue.quantity.value MEASUREMENT operator_concept_id, value_as_number, unit_concept_id measures.measurementValue.quantity.referenceRange MEASUREMENT All variables measures._info MEASUREMENT, PERSON measurement_date, birth_datetime measures.observationMoment MEASUREMENT measurement_date, birth_datetime measures.procedure.ageAtProcedure MEASUREMENT measurement_date measures.procedure.dateOfProcedure MEASUREMENT measurement_type_concept_id measures.procedure.prodecureCode MEASUREMENT person_id, visit_occurrence_id diseases._visit"},{"location":"omop2bff/#pedigrees","title":"pedigrees","text":"<p>NA</p>"},{"location":"omop2bff/#phenotypicfeatures","title":"phenotypicFeatures","text":"OMOP Table(s) OMOP Variable BFF JSON path OBSERVATION observation_concept_id phenotypicFeatures.featureType OBSERVATION All variables phenotypicFeatures._info OBSERVATION, PERSON observation_date, birth_datetime phenotypicFeatures.onset OBSERVATION person_id, visit_occurrence_id phenotypicFeatures._visit"},{"location":"omop2bff/#sex","title":"sex","text":"OMOP Table(s) OMOP Variable BFF JSON path PERSON gender_concept_id sex"},{"location":"omop2bff/#treatments","title":"treatments","text":"OMOP Table(s) OMOP Variable BFF JSON path DRUG_EXPOSURE, PERSON drug_exposure_start_date, birth_datetime treatments.ageOfOnset DEFAULT treatments.date DRUG_EXPOSURE All variables treatments._info DEFAULT treatments.doseIntervals DEFAULT treatments.routeOfAdministration DRUG_EXPOSURE drug_concept_id treatments.treatmentCode DRUG_EXPOSURE person_id, visit_occurrence_id treatments._visit <p>About <code>exposures</code></p> <p><code>exposures</code> terms are obtained from this CSV file. You can use a different csv file with the option <code>--exposures-file</code>.</p>"},{"location":"omop2bff/#last-change-2023-06-09-by-manuel-rueda","title":"last change 2023-06-09 by Manuel Rueda","text":""},{"location":"pxf/","title":"\ud83d\udce6 Phenopackets v2 (PXF)","text":"<p>PXF stands for Phenotype eXchange Format. Phenopackets v2 documentation.</p> <p> </p> Figure extracted from www.ga4gh.org <p>Phenopackets organize information using top-level elements. Our software, <code>Convert-Pheno</code>, specifically processes data from the Phenopacket element, serialized in PXF format.</p> Browsing PXF <code>JSON</code> data <p>You can browse a public Phenopackets v2 file with onf of teh following JSON viewers:</p> <ul> <li>JSON Crack</li> <li>JSON Hero</li> <li>Datasette</li> </ul>"},{"location":"pxf/#pxf-phenopacket-top-element-as-input","title":"PXF (Phenopacket top-element) as input","text":"Command-lineModuleAPI <p>When using the <code>convert-pheno</code> command-line interface, simply ensure the correct syntax is provided.</p> <pre><code>convert-pheno -ipxf ipxf.json -obff individuals.json\n</code></pre> About <code>JSON</code> data structure in I/O files <p>Note that the input <code>-ipxf</code> file can consist of one individual (one JSON object) or a list of individuals (a JSON array of objects). The output <code>--obff</code> file will replicate the data structure of the input file.</p> About <code>biosamples</code> and <code>interpretations</code> <p>If these properties are present, they will be included in <code>individuals.json</code> within the <code>info.phenopacket</code> field as unprocessed data, as they are not mapped to any specific entity within the Beacon v2 Models. See additional info here.</p> <p>The concept is to pass the necessary information as a hash (in Perl) or dictionary (in Python).</p> PerlPython <pre><code>$bff = {\n    data =&gt; $my_pxf_json_data,\n    method =&gt; 'pxf2bff'\n};\n</code></pre> <pre><code>bff = {\n     \"data\" : my_pxf_json_data,\n     \"method\" : \"pxf2bff\"\n}\n</code></pre> <p>The data will be sent as <code>POST</code> to the API's URL (see more info here). <pre><code>{\n\"data\": {...}\n\"method\": \"pxf2bff\"\n}\n</code></pre></p> <p>Please find below examples of data:</p> PXF (input)BFF (output) <pre><code>{\n   \"diseases\" : [],\n   \"id\" : \"phenopacket_id.AUNb6vNX1\",\n   \"measurements\" : [\n      {\n         \"assay\" : {\n            \"id\" : \"LOINC:35925-4\",\n            \"label\" : \"BMI\"\n         },\n         \"value\" : {\n            \"quantity\" : {\n               \"unit\" : {\n                  \"id\" : \"NCIT:C49671\",\n                  \"label\" : \"Kilogram per Square Meter\"\n               },\n               \"value\" : 26.63838307\n            }\n         }\n      },\n      {\n         \"assay\" : {\n            \"id\" : \"LOINC:3141-9\",\n            \"label\" : \"Weight\"\n         },\n         \"value\" : {\n            \"quantity\" : {\n               \"unit\" : {\n                  \"id\" : \"NCIT:C28252\",\n                  \"label\" : \"Kilogram\"\n               },\n               \"value\" : 85.6358\n            }\n         }\n      },\n      {\n         \"assay\" : {\n            \"id\" : \"LOINC:8308-9\",\n            \"label\" : \"Height-standing\"\n         },\n         \"value\" : {\n            \"quantity\" : {\n               \"unit\" : {\n                  \"id\" : \"NCIT:C49668\",\n                  \"label\" : \"Centimeter\"\n               },\n               \"value\" : 179.2973\n            }\n         }\n      }\n   ],\n   \"medicalActions\" : [\n      {\n         \"procedure\" : {\n            \"code\" : {\n               \"id\" : \"OPCS4:L46.3\",\n               \"label\" : \"OPCS(v4-0.0):Ligation of visceral branch of abdominal aorta NEC\"\n            },\n            \"performed\" : {\n               \"timestamp\" : \"1900-01-01T00:00:00Z\"\n            }\n         }\n      }\n   ],\n   \"metaData\" : null,\n   \"subject\" : {\n      \"id\" : \"HG00096\",\n      \"sex\" : \"MALE\",\n      \"vitalStatus\" : {\n         \"status\" : \"ALIVE\"\n      }\n   }\n}\n</code></pre> <pre><code>{\n  \"ethnicity\": {\n    \"id\": \"NCIT:C42331\",\n    \"label\": \"African\"\n  },\n  \"id\": \"HG00096\",\n  \"info\": {\n    \"eid\": \"fake1\"\n  },\n  \"interventionsOrProcedures\": [\n    {\n      \"procedureCode\": {\n        \"id\": \"OPCS4:L46.3\",\n        \"label\": \"OPCS(v4-0.0):Ligation of visceral branch of abdominal aorta NEC\"\n      }\n    }\n  ],\n  \"measures\": [\n    {\n      \"assayCode\": {\n        \"id\": \"LOINC:35925-4\",\n        \"label\": \"BMI\"\n      },\n      \"date\": \"2021-09-24\",\n      \"measurementValue\": {\n        \"quantity\": {\n          \"unit\": {\n            \"id\": \"NCIT:C49671\",\n            \"label\": \"Kilogram per Square Meter\"\n          },\n          \"value\": 26.63838307\n        }\n      }\n    },\n    {\n      \"assayCode\": {\n        \"id\": \"LOINC:3141-9\",\n        \"label\": \"Weight\"\n      },\n      \"date\": \"2021-09-24\",\n      \"measurementValue\": {\n        \"quantity\": {\n          \"unit\": {\n            \"id\": \"NCIT:C28252\",\n            \"label\": \"Kilogram\"\n          },\n          \"value\": 85.6358\n        }\n      }\n    },\n    {\n      \"assayCode\": {\n        \"id\": \"LOINC:8308-9\",\n        \"label\": \"Height-standing\"\n      },\n      \"date\": \"2021-09-24\",\n      \"measurementValue\": {\n        \"quantity\": {\n          \"unit\": {\n            \"id\": \"NCIT:C49668\",\n            \"label\": \"Centimeter\"\n          },\n          \"value\": 179.2973\n        }\n      }\n    }\n  ],\n  \"sex\": {\n    \"id\": \"NCIT:C20197\",\n    \"label\": \"male\"\n  }\n}\n</code></pre>"},{"location":"pxf2bff/","title":"\ud83d\udd04 PXF to BFF","text":"<p>PXF to BFF - Schemas</p> <ul> <li>Phenopacket v2 schema</li> <li>Beacon v2 Models - individuals</li> </ul> <p>Information</p> <p>The Beacon v2 schema enforces the presence of specific properties to achieve successful validation. In cases where no suitable match is found, DEFAULT values are employed to guarantee conformity.</p>"},{"location":"pxf2bff/#version-025","title":"Version 0.25","text":""},{"location":"pxf2bff/#terms","title":"Terms","text":""},{"location":"pxf2bff/#diseases","title":"diseases","text":"PXF JSON path BFF JSON path diseases diseases diseases.term diseases.diseaseCode diseases.onset diseases.ageOfOnset"},{"location":"pxf2bff/#ethnicity","title":"ethnicity","text":"<p>NA</p>"},{"location":"pxf2bff/#exposures","title":"exposures","text":"PXF JSON path BFF JSON path exposures exposures exposures.type exposures.exposureCode exposures.occurrence.timestamp exposures.date"},{"location":"pxf2bff/#geographicorigin","title":"geographicOrigin","text":"<p>NA</p>"},{"location":"pxf2bff/#id","title":"id","text":"PXF JSON path BFF JSON path subject.id id"},{"location":"pxf2bff/#info","title":"info","text":"PXF JSON path BFF JSON path dateOfBirth, genes, metaData, variants, interpretations, files, biosample info"},{"location":"pxf2bff/#interventionsorprocedures","title":"interventionsOrProcedures","text":"PXF JSON path BFF JSON path medicalActions.procedure interventionsOrProcedures medicalActions.procedure.code interventionsOrProcedures.procedureCode medicalActions.procedure.performed interventionsOrProcedures.ageAtProcedure"},{"location":"pxf2bff/#karyotypicsex","title":"karyotypicSex","text":"PXF JSON path BFF JSON path subject.karyotypicSex karyotypicSex"},{"location":"pxf2bff/#measures","title":"measures","text":"PXF JSON path BFF JSON path measurements measures measurements.assay measures.assayCode measurements.value measures.measurementValue measurements.complexValue.typedQuantities.type measures.measurementValue.typedQuantities.quantityType measurements.timeObserved measures.observationMoment measurements.procedure measures.procedure"},{"location":"pxf2bff/#pedigrees","title":"pedigrees","text":"<p>NA</p>"},{"location":"pxf2bff/#phenotypicfeatures","title":"phenotypicFeatures","text":"PXF JSON path BFF JSON path phenotypicFeatures phenotypicFeatures phenotypicFeatures.type phenotypicFeatures.featureType phenotypicFeatures.negated phenotypicFeatures.excluded phenotypicFeatures.evidence (array) phenotypicFeatures.evidence (v2.0.0 is still object)"},{"location":"pxf2bff/#sex","title":"sex","text":"PXF JSON path BFF JSON path subject.sex sex"},{"location":"pxf2bff/#treatments","title":"treatments","text":"PXF JSON path BFF JSON path medicalActions.treatment treatments medicalActions.treatment.agent treatments.treatmentCode"},{"location":"pxf2bff/#last-change-2023-03-09-by-manuel-rueda","title":"last change 2023-03-09 by Manuel Rueda","text":""},{"location":"redcap/","title":"\ud83d\udcdd REDCap","text":"<p>Experimental</p> <p>REDCap conversion is still experimental. Please us it with caution.</p> <p>REDCap stands for Research Electronic Data Capture. REDCap documentation.</p>"},{"location":"redcap/#redcap-as-input","title":"REDCap as input","text":"<p>REDCap projects are inherently \"free format\", meaning the project creator has the flexibility to determine the identifiers for variables, data dictionaries, and other elements.</p> <p>REDCap project creation user\u2019s guide</p> <p>\u201cWe always recommend reviewing your variable names with a statistician or whoever will be analyzing your data. This is especially important if this is the first time you are building a database.\u201d </p> <p>Due to the flexibility of REDCap projects, it can be challenging to develop a solution that accommodates the wide range of possibilities. Nonetheless, we were able to successfully convert data from REDCap project exports to both Beacon v2 and Phenopackets v2 formats using a mapping file. These conversions were achieved as part of the 3TR Project.</p> About REDCap longitudinal data <p>REDCap stores <code>event</code> information, however, Beacon v2 Models currently lack a way to store longitudinal data. To address this, we will store <code>event</code> data under the propery <code>info</code>.</p> Command-lineAPI About REDCap export formats <p>REDCap provides various options for exporting data. We accept the option \"All data (all records and fields)\" including CSV and Microsoft Excel format, along with a accompanying data dictionary in CSV format. Exportation in REDCap CDISC ODM (XML) format is discussed in the section on CDISC-ODM.</p> <p>We'll need three files:</p> <ol> <li>REDCap export (CSV)</li> <li>REDCap data dictionary (CSV)</li> <li>Mapping file (YAML or JSON) (see tutorial)</li> </ol> Can CSV files be compressed? <p>Yes. We also accept as input files compressed with <code>gzip</code>.</p> <pre><code>convert-pheno -iredcap redcap.csv --redcap-dictionary dictionary.csv --mapping-file mapping.yaml -obff individuals.json\n</code></pre> <p>While it is technically possible to perform a transformation via API we don't think it's a viable option with REDCap projects due to the need for loading the data dictionary and mapping files along with the data. Therefore, we recommend using the command-line version by utilizing REDCap data exports.</p> REDCap built in API <p>REDCap has a built-in API that in theory could be used to retrieve data in real-time (as opposed to data exports). However, the current version of <code>Convert-Pheno</code> does not support REDCap API calls.</p> Input CLI UI Module API Beacon v2 Models YES YES YES YES CDISC-ODM YES YES YES NO CSV YES NO YES NO Phenopackets v2 YES YES YES YES OMOP-CDM YES YES YES YES REDCap YES YES YES NO"},{"location":"supported-formats/","title":"Supported formats","text":"%%{init:{'theme':'neutral'}}%% graph LR   subgraph \"\ud83c\udfaf Target Model\"     A[\ud83e\uddec Beacon v2 Models]   end    A --&gt;|bff2pxf| B[\ud83d\udce6 Phenopackets v2]   C[\ud83d\udcdd REDCap] --&gt;|redcap2pxf| B   D[\ud83c\udfe5 OMOP-CDM] --&gt;|omop2bff| A   E[\ud83d\udcd1 CDISC-ODM] --&gt;|cdisc2bff| A   F[\ud83d\udcca CSV] --&gt;|csv2bff| A   B --&gt;|pxf2bff| A   C --&gt;|redcap2bff| A   D --&gt;|omop2pxf| B   E --&gt;|cdisc2pxf| B   F --&gt;|csv2pxf| B   A --&gt;|\ud83e\uddea bff2omop| D   F --&gt;|\ud83e\uddea csv2omop| D   E --&gt;|\ud83e\uddea cdisc2omop| D   C --&gt;|\ud83e\uddea redcap2omop| D   B --&gt;|\ud83e\uddea pxf2omop| D     style A fill:#6495ED,stroke:#6495ED   style B fill:#FF7F50,stroke:#FF7F50   style C fill:#FF6965,stroke:#FF6965   style D fill:#3CB371,stroke:#3CB371   style E fill:#DDA0DD,stroke:#DDA0DD   style F fill:#FFFF00,stroke:#FFFF00    %% dotted, thinner, semi\u2011transparent blue for the \u201cmust via BFF\u201d shortcuts      linkStyle 1,7,8,9,11,12,13 stroke:#0000FF,stroke-width:1px,stroke-dasharray:5 5,opacity:0.5;  Convert-Pheno supported data conversions (Apr-17-2025).Note: Dotted blue lines \u2192 must go via Beacon v2 Models Input formats:Output formats: <ul> <li>Beacon v2 Models (JSON | YAML)</li> <li>Phenopacket v2 (JSON | YAML)</li> <li>OMOP-CDM (SQL export | CSV)</li> <li>\ud83e\uddea REDCap exports (CSV)</li> <li>\ud83e\uddea CDISC-ODM v1 (XML)</li> <li>\ud83e\uddea CSV raw data</li> </ul>"},{"location":"supported-formats/#main","title":"Main","text":"<ul> <li>Beacon v2 Models (JSON | YAML)</li> <li>Phenopacket v2 (JSON | YAML)</li> <li>\ud83e\uddea OMOP CDM (CSV)</li> </ul> Why GA4GH Beacon v2 and Phenopackets v2? <p>Beacon v2 [BFF] and Phenopackets v2 [PXF] are data exchange standards from the GA4GH. They:</p> <ul> <li>Allow for storing both phenotypic and genomic data, a key component in today's research</li> <li>Facilitate streamlined data representation in genomic and biomedical research environments </li> <li>Play a central role in mapping exercises due to their structured and compact data schemas</li> <li>Are not intended to replace other health data</li> <li>Foster effective data sharing and integration initiatives</li> </ul>"},{"location":"supported-formats/#additional-output-formats","title":"Additional Output Formats","text":"<p>Given that Beacon v2 Models and Phenopackets v2 utilize JSON to encode data in a complex tree-like structure, this format presents challenges for straightforward analytics. To mitigate this and enhance data usability, we provide options to convert from <code>BFF/PXF</code> to more analytics-friendly formats:</p> <ul> <li>\"Flattened\" (a.k.a., folded) JSON or YAML with the option <code>--ojsonf</code></li> <li>CSV with the option <code>--ocsv</code></li> </ul> <p>Additionally, we are working on a conversion to JSON-LD, a format that is compatible with the RDF ecosystem, used in many healthcare-related data systems.</p> <ul> <li>\ud83e\uddea JSON-LD (or YAML-LD) with the option <code>--jsonld</code></li> </ul> <p>Hint</p> <p>Note that you can convert from any accepted input format to either <code>BFF</code> or <code>PXF</code>.</p> <p>%%{init: {'theme':'neutral'}}%% graph LR    A[\ud83e\uddec BFF] --&gt;|bff2jsonf| C[\ud83e\uddfe JSON Flattened];   A --&gt;|bff2csv| D[\ud83d\udcca CSV];   A --&gt;|bff2jsonld| E[\ud83d\udd17 JSON-LD];    B[\ud83d\udce6 PXF] --&gt;|pxf2jsonf| C;   B --&gt;|pxf2csv| D;   B --&gt;|pxf2jsonld| E;    style A fill: #6495ED   style A stroke: #6495ED   style B fill: #FF7F50   style B stroke: #FF7F50   style C fill: #FFFF00   style C stroke: #FFFF00   style D fill: #EOEOEO   style D stroke: #EOEOEO   style E fill: #9999FF   style E stroke: #9999FF  Convert-Pheno additional data conversions</p>"},{"location":"tutorial/","title":"\ud83c\udf93 Tutorial","text":"Google Colab version <p>We created a Google Colab version of the tutorial. Users can view notebooks shared publicly without sign-in, but you need a google account to execute code.</p> <p> </p> <p>We also have a local copy of the notebook that can be downloaded from the repo. </p> <p>This page provides brief tutorials on how to perform data conversion by using <code>Convert-Pheno</code>command-line interface.</p> Note on installation <p>Before proceeding, ensure that the software is properly installed. In the following instructions, it will be assumed that you have downloaded and installed Convert-Pheno.</p>"},{"location":"tutorial/#how-to-convert","title":"How to convert:","text":"REDCap to Phenopackets v2OMOP CDM to Beacon v2 ModelsCSV to Beacon v2 Models <p>This section provides a summary of the steps to convert a REDCap project to Phenopackets v2. </p> <ul> <li>The starting point is to log in to your REDCap system and export the data to CSV / Microsoft Excel (raw data) format. If you need more information on REDCap, we recommend consulting the comprehensive documentation provided by the Cincinnati Children's Hospital Medical Center.</li> </ul> Can I export CSV / Microsoft Excel (labels) file? <p>Yes, you can export a CSV or Microsoft Excel file with labels. However, you need to use the <code>--icsv</code> flag instead of the <code>--iredcap</code> flag as the input format. While we recommend exporting raw data along with the dictionary for better accuracy, we understand that this might not always be possible.</p> <p>For more detailed information and other common questions, please refer to the FAQ.</p> <ul> <li> <p>After exporting the data, you must also download the REDCap dictionary in CSV format. This can be done within REDCap by navigating to <code>Project Setup/Data Dictionary/Download the current</code>.</p> </li> <li> <p>Since REDCap projects are \"free-format,\" a mapping file is necessary to connect REDCap project variables (i.e. fields) to something meaningful for <code>Convert-Pheno</code>. This mapping file will be used in the conversion process.</p> </li> </ul> What is a <code>Convert-Pheno</code> mapping file? <p>A mapping file is a text file in YAML format (JSON is also accepted) that connects a set of variables to a format that is understood by <code>Convert-Pheno</code>. This file maps your variables to the required terms of the individuals entity from the Beacon v2 models, which serves a center model.</p> About text similarity in database searches <p><code>Convert-Pheno</code> comes with several pre-configured ontology/terminology databases. It supports three types of label-based search strategies:</p> Composite Similarity Score <p>The composite similarity score is computed as a weighted sum of two measures: the token-based similarity and the normalized Levenshtein similarity.</p> <p>This section provides a summary of the steps to convert an OMOP CDM export to Beacon v2 Models. The starting point is either a PostgreSQL export in the form of <code>.sql</code> or <code>.csv</code> files. The process is the same for both.</p> <p>Two possibilities may arise:</p> <ol> <li>Full export of records.</li> <li>Partial export of records.</li> </ol> <p>This section provides a summary of the steps to convert a CSV file with raw clinical data to Phenopackets v2.</p> <ul> <li>Since CSV files  are \"free-format,\" a mapping file is necessary to connect variables (i.e. fields) to something meaningful for <code>Convert-Pheno</code>. This mapping file will be used in the conversion process.</li> </ul> What is a <code>Convert-Pheno</code> mapping file? <p>A mapping file is a text file in YAML format (JSON is also accepted) that connects a set of variables to a format that is understood by <code>Convert-Pheno</code>. This file maps your variables to the required terms of the individuals entity from the Beacon v2 models, which serves a center model.</p> <p>More questions?</p> <p>Please take a look to our Frequently Asked Questions.</p>"},{"location":"tutorial/#creating-a-mapping-file","title":"Creating a mapping file","text":"<p>To create a mapping file, start by reviewing the example mapping file provided with the installation. The goal is to replace the contents of such file with those from your REDCap project. The mapping file contains the following types of data:</p> Type Required (Optional) Required properties Optional properties Internal <code>project</code> <code>id, source, ontology, version</code> <code>description, baselineFieldsToPropagate</code> Beacon v2 terms <code>id, sex (diseases, exposures, info, interventionsOrProcedures, measures, phenotypicFeatures, treatments)</code> <code>fields</code> <code>age,ageOfOnset,assignTermIdFromHeader,bodySite,dateOfProcedure,dictionary,drugDose,drugUnit,duration,durationUnit,familyHistory,fields,mapping,procedureCodeLabel,selector,terminology,unit,visitId</code> <p>These are the properties needed to map your data to the entity <code>individuals</code> in the Beacon v2 Models:</p> <ul> <li>baselineFieldsToPropagate, an array of columns containing measurements that were taken only at the initial time point (time = 0). Use this if you wish to duplicate these columns across subsequent rows for the same patient ID. It is important to ensure that the row containing baseline information appears first in the CSV.</li> <li>age, a <code>string</code> representing the column that points to the age of the patient.</li> <li>ageOfOnset, an <code>object</code> representing the column that points to the age at which the patient first experienced symptoms or was diagnosed with a condition.</li> <li>assignTermIdFromHeader, an <code>array</code> for columns on which the ontology-term ids have to be assigned from the header.</li> <li>bodySite, an <code>object</code> representing the column that points to the part of the body affected by a condition or where a procedure was performed.</li> <li>dateOfProcedure, an <code>object</code> representing the column that points to when a procedure took place.</li> <li>dictionary, is an <code>object</code> in the form of <code>key: value</code>. The <code>key</code> represents the original variable name in REDCap and the <code>value</code> represents the \"phrase\" that will be used to query a database to find an ontology candidate. For instance, you may have a variable named <code>cigarettes_days</code>, but you know that in NCIt the label is <code>Average Number Cigarettes Smoked a Day</code>. In this case, you will use <code>cigarettes_days: Average Number Cigarettes Smoked a Day</code>.</li> <li>drugDose, an <code>object</code> representing the column that points to the dose column for each treatment.</li> <li>drugUnit, an <code>object</code> representing the column that points to the unit column for each treatment.</li> <li>duration, an <code>object</code> representing the column that points to the duration column for each treatment.</li> <li>durationUnit, an <code>object</code> representing the column that points to the duration unit column for each treatment.</li> <li>familyHistory, an <code>object</code> representing the column that points to the family medical history relevant to the patient's condition.</li> <li>fields, can be either a <code>string</code> or an <code>array</code> consisting of the name of the REDCap variables that map to that Beacon v2 term.</li> <li>mapping, is an <code>object</code> in the form of <code>key: value</code> that we use to map our Beacon v2 objects to REDCap variables.</li> <li>procedureCodeLabel , a nested <code>object</code> with specific mappings for <code>interventionsOrProcedures</code>.</li> <li>ontology, it's an <code>string</code> to define more granularly the ontology for this particular Beacon v2 term. If not present, the script will use that from <code>project.ontology</code>.</li> <li>routeOfAdministration, a nested <code>object</code> with specific mappings for <code>treatments</code>.</li> <li>selector, a nested <code>object</code> value with specific mappings.</li> <li>terminology, a nested <code>object</code> value with user-defined ontology terms.</li> </ul> Terminology example <pre><code>terminology:\n  My fav term:\n    id: FOO:12345678\nlabel: Label for my fav term\n</code></pre> <ul> <li>unit, an <code>object</code> representing the column that points to the unit of measurement for a given value or treatment.</li> <li>visitId, the column with visit occurrence id.</li> </ul> Defining the values in the property <code>dictionary</code> <p>Before assigning values to <code>dictionary</code> it's important that you think about which ontologies/terminologies you want to use. The field <code>project.ontology</code> defines the ontology for the whole project, but you can also specify a another antology at the Beacon v2 term level. Once you know which ontologies to use, then try searching for such term to get an accorate label for it. For example, if you have chosen <code>ncit</code>, you can search for the values within NCIt at EBI Search. <code>Convert-Pheno</code> will use these values to retrieve the actual ontology term from its internal databases.</p>"},{"location":"tutorial/#1-exact-default","title":"1. <code>exact</code> (default)","text":"<p>Returns only exact matches for the given label string. If the label is not found exactly, no results are returned.</p>"},{"location":"tutorial/#2-mixed-use-search-mixed","title":"2. <code>mixed</code> (use <code>--search mixed</code>)","text":"<p>Hybrid search: First tries to find an exact label match. If none is found, it performs a token-based similarity search and returns the closest matching concept based on the highest similarity score.</p>"},{"location":"tutorial/#3-fuzzy-use-search-fuzzy","title":"3. \u2728 <code>fuzzy</code> (use <code>--search fuzzy</code>)","text":"<p>Hybrid search with fuzzy ranking: Like <code>mixed</code>, it starts with an exact match attempt. If that fails, it performs a weighted similarity search, where: - 90% of the score comes from token-based similarity (e.g., cosine or Dice coefficient), - 10% comes from the normalized Levenshtein similarity.</p> <p>The concept with the highest composite score is returned.</p> <p>Note: The normalized Levenshtein similarity is computed on top of the candidate results produced by the full text search. In this approach, an initial full text search (using token-based methods) returns a set of potential matches. The fuzzy search then refines these results by applying the normalized Levenshtein distance to better handle minor typographical differences, ensuring that the final composite score reflects both overall token similarity and fine-grained character-level differences.</p>"},{"location":"tutorial/#example-search-behavior","title":"\ud83d\udd0d Example Search Behavior","text":"<p>Query: <code>Exercise pain management</code> - With <code>--search exact</code>: \u2705 Match found \u2014 Exercise Pain Management</p> <p>Query: <code>Brain Hemorrhage</code> - With <code>--search mixed</code>:   - \u274c No exact match   - \u2705 Closest match by similarity: Intraventricular Brain Hemorrhage</p>"},{"location":"tutorial/#similarity-threshold","title":"\ud83d\udca1 Similarity Threshold","text":"<p>The <code>--min-text-similarity-score</code> option sets the minimum threshold for <code>mixed</code> and <code>fuzzy</code> searches. - Default: <code>0.8</code> (conservative) - Lowering the threshold may increase recall but may introduce irrelevant matches.</p>"},{"location":"tutorial/#performance-note","title":"\u26a0\ufe0f Performance Note","text":"<p>Both <code>mixed</code> and <code>fuzzy</code> modes are more computationally intensive and can produce unexpected or less interpretable matches. Use them with care, especially on large datasets.</p>"},{"location":"tutorial/#example-results-table","title":"\ud83e\uddea Example Results Table","text":"<p>Below is an example showing how the query <code>Sudden Death Syndrome</code> performs using different search modes against the NCIt ontology:</p> Query Search NCIt match (label) NCIt code Cosine Dice Levenshtein (Normalized) Composite Sudden Death Syndrome exact NA NA NA NA NA NA mixed CDISC SDTM Sudden Death Syndrome Type Terminology NCIT:C101852 0.65 0.60 NA NA Family History of Sudden Arrythmia Death Syndrome NCIT:C168019 0.65 0.60 NA NA Family History of Sudden Infant Death Syndrome NCIT:C168209 0.65 0.60 NA NA Sudden Infant Death Syndrome NCIT:C85173 0.86 0.86 NA NA \u2728 fuzzy CDISC SDTM Sudden Death Syndrome Type Terminology NCIT:C101852 0.65 0.60 0.43 0.63 Family History of Sudden Arrythmia Death Syndrome NCIT:C168019 0.65 0.60 0.43 0.63 Family History of Sudden Infant Death Syndrome NCIT:C168209 0.65 0.60 0.46 0.63 Sudden Infant Death Syndrome NCIT:C85173 0.86 0.86 0.75 0.85 <p>Interpretation: </p> <ul> <li> <p>With <code>exact</code>, there are no matches.</p> </li> <li> <p>With <code>mixed</code>, the best match will be <code>Sudden Infant Death Syndrome</code>.</p> </li> <li> <p>With <code>fuzzy</code>, the composite score (90% token-based + 10% Levenshtein similarity) is used to rank results.   The highest match is <code>Sudden Infant Death Syndrome</code>, with a composite score of 0.85.</p> </li> </ul> <p>\u2728 Now we introduce a typo on the query <code>Sudden Infant Deth Syndrome</code>:</p> Query Mode Candidate Label Code Cosine Dice Levenshtein (Normalized) Composite Sudden Infant Deth Syndrome fuzzy CDISC SDTM Sudden Death Syndrome Type Terminology NCIT:C101852 0.38 0.36 0.33 0.37 Family History of Sudden Arrythmia Death Syndrome NCIT:C168019 0.38 0.36 0.43 0.38 Family History of Sudden Infant Death Syndrome NCIT:C168209 0.57 0.55 0.59 0.57 Sudden Infant Death Syndrome NCIT:C85173 0.75 0.75 0.96 0.77 <p>To capture the best match we would need to lower the threshold to  <code>--min-text-similarity-score 0.75</code></p> <p>It is possible to change the weight of Levenshtein similarity via <code>--levenshtein-weight &lt;floating 0.0 - 1.0&gt;</code>.</p>"},{"location":"tutorial/#1-token-based-similarity","title":"1. Token-Based Similarity","text":"<p>This is calculated using methods like cosine or Dice similarity to measure how similar the tokens (words) of two strings are.</p>"},{"location":"tutorial/#2-normalized-levenshtein-similarity","title":"2. Normalized Levenshtein Similarity","text":"<p>The normalized Levenshtein similarity is defined as:</p> \\[ \\text{NormalizedLevenshtein}(s_1, s_2) = 1 - \\frac{\\text{lev}(s_1, s_2)}{\\max(|s_1|, |s_2|)} \\] <p>Where: - \\(\\text{lev}(s_1, s_2)\\) is the Levenshtein edit distance\u2014the minimum number of insertions, deletions, or substitutions required to change \\(s_1\\) into \\(s_2\\). - \\(|s_1|\\) and \\(|s_2|\\) are the lengths of the strings \\(s_1\\) and \\(s_2\\), respectively.</p> <p>This formula produces a score between 0 and 1, with 1.0 meaning identical strings and 0.0 meaning completely different strings.</p>"},{"location":"tutorial/#3-composite-score-formula","title":"3. Composite Score Formula","text":"<p>The final composite similarity score \\(C\\) is a weighted combination of the two metrics:</p> \\[ C(s_1, s_2) = \\alpha \\cdot \\text{TokenSimilarity}(s_1, s_2) + \\beta \\cdot \\text{NormalizedLevenshtein}(s_1, s_2) \\] <p>Where: - \\(\\alpha\\) (or <code>token_weight</code>) is the weight assigned to the token-based similarity. - \\(\\beta\\) (or <code>lev_weight</code>) is the weight assigned to the normalized Levenshtein similarity.</p> <p>A common default is to set \\(\\alpha = 0.9\\) and \\(\\beta = 0.1\\), emphasizing the token-based similarity. However, for short strings (4\u20135 words), you might consider adjusting the balance (for example, \\(\\alpha = 0.95\\) and \\(\\beta = 0.05\\)) if small typographical differences are less critical.</p>"},{"location":"tutorial/#running-convert-pheno","title":"Running <code>Convert-Pheno</code>","text":"<p>Now you can proceed to run <code>convert-pheno</code> with the command-line interface. Please see how here.</p>"},{"location":"tutorial/#full-export","title":"Full export","text":"<p>In a full export, all standardized terms are included in the <code>CONCEPT</code> table, thus Convert-Pheno does not need to search any additional databases for terminology (with a few exceptions). </p>"},{"location":"tutorial/#partial-export","title":"Partial export","text":"<p>In a partial export, many standardized terms may be missing from the <code>CONCEPT</code> table, as a result, <code>Convert-Pheno</code> will perform a search on the included ATHENA-OHDSI database. To enable this search you should use the flag <code>--ohdsi-db</code>.</p>"},{"location":"tutorial/#running-convert-pheno_1","title":"Running <code>Convert-Pheno</code>","text":"<p>Now you can proceed to run <code>convert-pheno</code> with the command-line interface. Please see how here.</p>"},{"location":"tutorial/#creating-a-mapping-file_1","title":"Creating a mapping file","text":"<p>To create a mapping file, start by reviewing the example mapping file provided with the installation. The goal is to replace the contents of such file with those from your REDCap project. The mapping file contains the following types of data:</p> Type Required (Optional) Required properties Optional properties Internal <code>project</code> <code>id, source, ontology, version</code> <code>description, baselineFieldsToPropagate</code> Beacon v2 terms <code>id, sex (diseases, exposures, info, interventionsOrProcedures, measures, phenotypicFeatures, treatments)</code> <code>fields</code> <code>age,ageOfOnset,assignTermIdFromHeader,bodySite,dateOfProcedure,dictionary,drugDose,drugUnit,duration,durationUnit,familyHistory,fields,mapping,procedureCodeLabel,selector,terminology,unit,visitId</code> <p>These are the properties needed to map your data to the entity <code>individuals</code> in the Beacon v2 Models:</p> <ul> <li>baselineFieldsToPropagate, an array of columns containing measurements that were taken only at the initial time point (time = 0). Use this if you wish to duplicate these columns across subsequent rows for the same patient ID. It is important to ensure that the row containing baseline information appears first in the CSV.</li> <li>age, a <code>string</code> representing the column that points to the age of the patient.</li> <li>ageOfOnset, an <code>object</code> representing the column that points to the age at which the patient first experienced symptoms or was diagnosed with a condition.</li> <li>assignTermIdFromHeader, an <code>array</code> for columns on which the ontology-term ids have to be assigned from the header.</li> <li>bodySite, an <code>object</code> representing the column that points to the part of the body affected by a condition or where a procedure was performed.</li> <li>dateOfProcedure, an <code>object</code> representing the column that points to when a procedure took place.</li> <li>dictionary, is an <code>object</code> in the form of <code>key: value</code>. The <code>key</code> represents the original variable name in REDCap and the <code>value</code> represents the \"phrase\" that will be used to query a database to find an ontology candidate. For instance, you may have a variable named <code>cigarettes_days</code>, but you know that in NCIt the label is <code>Average Number Cigarettes Smoked a Day</code>. In this case, you will use <code>cigarettes_days: Average Number Cigarettes Smoked a Day</code>.</li> <li>drugDose, an <code>object</code> representing the column that points to the dose column for each treatment.</li> <li>drugUnit, an <code>object</code> representing the column that points to the unit column for each treatment.</li> <li>duration, an <code>object</code> representing the column that points to the duration column for each treatment.</li> <li>durationUnit, an <code>object</code> representing the column that points to the duration unit column for each treatment.</li> <li>familyHistory, an <code>object</code> representing the column that points to the family medical history relevant to the patient's condition.</li> <li>fields, can be either a <code>string</code> or an <code>array</code> consisting of the name of the REDCap variables that map to that Beacon v2 term.</li> <li>mapping, is an <code>object</code> in the form of <code>key: value</code> that we use to map our Beacon v2 objects to REDCap variables.</li> <li>procedureCodeLabel , a nested <code>object</code> with specific mappings for <code>interventionsOrProcedures</code>.</li> <li>ontology, it's an <code>string</code> to define more granularly the ontology for this particular Beacon v2 term. If not present, the script will use that from <code>project.ontology</code>.</li> <li>routeOfAdministration, a nested <code>object</code> with specific mappings for <code>treatments</code>.</li> <li>selector, a nested <code>object</code> value with specific mappings.</li> <li>terminology, a nested <code>object</code> value with user-defined ontology terms.</li> </ul> Terminology example <pre><code>terminology:\n  My fav term:\n    id: FOO:12345678\nlabel: Label for my fav term\n</code></pre> <ul> <li>unit, an <code>object</code> representing the column that points to the unit of measurement for a given value or treatment.</li> <li>visitId, the column with visit occurrence id.</li> </ul> Defining the values in the property <code>dictionary</code> <p>Before assigning values to <code>dictionary</code> it's important that you think about which ontologies/terminologies you want to use. The field <code>project.ontology</code> defines the ontology for the whole project, but you can also specify a another antology at the Beacon v2 term level. Once you know which ontologies to use, then try searching for such term to get an accorate label for it. For example, if you have chosen <code>ncit</code>, you can search for the values within NCIt at EBI Search. <code>Convert-Pheno</code> will use these values to retrieve the actual ontology term from its internal databases.</p>"},{"location":"tutorial/#running-convert-pheno_2","title":"Running <code>Convert-Pheno</code>","text":"<p>Now you can proceed to run <code>convert-pheno</code> with the command-line interface. Please see how here.</p>"},{"location":"usage/","title":"\ud83d\udee0 Usage","text":"<p> A software toolkit for the interconversion of standard data models for phenotypic data </p> <p> </p> <p>\ud83d\udcd8 Documentation: cnag-biomedical-informatics.github.io/convert-pheno</p> <p>\ud83d\udcd3 Google Colab tutorial: colab.research.google.com/drive/1T6F3bLwfZyiYKD6fl1CIxs9vG068RHQ6?usp=sharing</p> <p>\ud83d\udce6 CPAN Distribution: metacpan.org/pod/Convert::Pheno</p> <p>\ud83d\udc33 Docker Hub Image: hub.docker.com/r/manuelrueda/convert-pheno/tags</p> <p>\ud83c\udf10 Web App UI: convert-pheno.cnag.cat</p>"},{"location":"usage/#table-of-contents","title":"Table of contents","text":"<ul> <li>Description</li> <li>Name</li> <li>Synopsis</li> <li>Summary</li> <li>Installation</li> <li>Non-Containerized</li> <li>Containerized</li> <li>How to run convert-pheno</li> <li>Citation</li> <li>Author</li> <li>License</li> </ul>"},{"location":"usage/#name","title":"NAME","text":"<p>convert-pheno - A script to interconvert common data models for phenotypic data</p>"},{"location":"usage/#synopsis","title":"SYNOPSIS","text":"<pre><code>convert-pheno [-i input-type] &lt;infile&gt; [-o output-type] &lt;outfile&gt; [-options]\n\n    Arguments:                       \n      (input-type): \n            -ibff                    Beacon v2 Models ('individuals' JSON|YAML) file\n            -iomop                   OMOP-CDM CSV files or PostgreSQL dump\n            -ipxf                    Phenopacket v2 (JSON|YAML) file\n            -iredcap (experimental)  REDCap (raw data) export CSV file\n            -icdisc  (experimental)  CDISC-ODM v1 XML file\n            -icsv    (experimental)  Raw data CSV\n\n            (Wish-list)\n            #-iopenehr               openEHR\n\n      (output-type):\n            -obff                    Beacon v2 Models ('individuals' JSON|YAML) file\n            -opxf                    Phenopacket v2 (JSON|YAML) file\n            -oomop   (experimental)  Prefix for OMOP-CDM tables (CSV)\n\n            Compatible with -i(bff|pxf):\n            -ocsv                    Flatten data to CSV\n            -ojsonf                  Flatten data to 1D-JSON (or 1D-YAML if suffix is .yml|.yaml)\n            -ojsonld (experimental)  JSON-LD (interoperable w/ RDF ecosystem; YAML-LD if suffix is .ymlld|.yamlld)\n\n    Options:\n      -exposures-file &lt;file&gt;         CSV file with a list of 'concept_id' considered to be exposures (with -iomop)\n      -levenshtein-weight &lt;weight&gt;   Set the normalized Levenshtein weight for fuzzy search composite scoring (default: 0.1, range: 0-1)\n      -mapping-file &lt;file&gt;           Fields mapping YAML (or JSON) file\n      -max-lines-sql &lt;number&gt;        Maximum lines read per table from SQL dump [500]\n      -min-text-similarity-score &lt;score&gt; Minimum score for cosine similarity (or Sorensen-Dice coefficient) [0.8] (to be used with --search [mixed|fuzzy])\n      -ohdsi-db                      Use Athena-OHDSI database (~2.2GB) with -iomop\n      -omop-tables &lt;tables&gt;          OMOP-CDM tables to be processed. Tables &lt;CONCEPT&gt; and &lt;PERSON&gt; are always included.\n      -out-dir &lt;directory&gt;           Output (existing) directory\n      -O                             Overwrite output file\n      -path-to-ohdsi-db &lt;directory&gt;  Directory for the file &lt;ohdsi.db&gt;\n      -phl|print-hidden-labels       Print original values (before DB mapping) of text fields &lt;_labels&gt;\n      -rcd|redcap-dictionary &lt;file&gt;  REDCap data dictionary CSV file\n      -schema-file &lt;file&gt;            Alternative JSON Schema for mapping file\n      -search &lt;type&gt;                 Type of search [&gt;exact|mixed|fuzzy]\n      -svs|self-validate-schema      Perform a self-validation of the JSON schema that defines mapping (requires IO::Socket::SSL)\n      -sep|separator &lt;char&gt;          Delimiter character for CSV files [;] e.g., --sep $'\\t'\n      -stream                        Enable incremental processing with -iomop and -obff [&gt;no-stream|stream]\n      -sql2csv                       Print SQL TABLES (only valid with -iomop). Mutually exclusive with --stream\n      -test                          Does not print time-changing-events (useful for file-based cmp)\n      -text-similarity-method &lt;method&gt; The method used to compare values to DB [&gt;cosine|dice]\n      -u|username &lt;username&gt;         Set the username\n\n    Generic Options:\n      -debug &lt;level&gt;                 Print debugging level (from 1 to 5, being 5 max)\n      -help                          Brief help message\n      -log                           Save log file (JSON). If no argument is given then the log is named [convert-pheno-log.json]\n      -man                           Full documentation\n      -no-color                      Don't print colors to STDOUT [&gt;color|no-color]\n      -v|verbose                     Verbosity on\n      -V|version                     Print Version\n</code></pre>"},{"location":"usage/#description","title":"DESCRIPTION","text":"<p><code>convert-pheno</code> is a command-line front-end to the CPAN's module Convert::Pheno.</p>"},{"location":"usage/#summary","title":"SUMMARY","text":"<p><code>convert-pheno</code> is a command-line front-end to the CPAN's module Convert::Pheno to interconvert common data models for phenotypic data.</p>"},{"location":"usage/#installation","title":"INSTALLATION","text":"<p>If you plan to only use the CLI, we recommend installing it via CPAN. See details below.</p>"},{"location":"usage/#non-containerized","title":"Non containerized","text":"<p>The script runs on command-line Linux and it has been tested on Debian/RedHat/MacOS based distributions (only showing commands for Debian's). Perl 5 is installed by default on Linux,  but we will install a few CPAN modules with <code>cpanminus</code>.</p>"},{"location":"usage/#method-1-from-cpan","title":"Method 1: From CPAN","text":"<p>First install system level dependencies:</p> <pre><code>sudo apt-get install cpanminus libbz2-dev zlib1g-dev libperl-dev libssl-dev\n</code></pre> <p>Now you have two choose between one of the 2 options below:</p> <p>Option 1: Install Convert-Pheno and the dependencies at <code>~/perl5</code></p> <pre><code>cpanm --local-lib=~/perl5 local::lib &amp;&amp; eval $(perl -I ~/perl5/lib/perl5/ -Mlocal::lib)\ncpanm --notest Convert::Pheno\nconvert-pheno --help\n</code></pre> <p>To ensure Perl recognizes your local modules every time you start a new terminal, you should type:</p> <pre><code>echo 'eval $(perl -I ~/perl5/lib/perl5/ -Mlocal::lib)' &gt;&gt; ~/.bashrc\n</code></pre> <p>To update to the newest version:</p> <pre><code>cpanm Convert::Pheno\n</code></pre> <p>Option 2: Install Convert-Pheno and the dependencies in a \"virtual environment\" (at <code>local/</code>) . We'll be using the module <code>Carton</code> for that:</p> <pre><code>mkdir local\ncpanm --notest --local-lib=local/ Carton\necho \"requires 'Convert::Pheno';\" &gt; cpanfile\nexport PATH=$PATH:local/bin; export PERL5LIB=$(pwd)/local/lib/perl5:$PERL5LIB\ncarton install\ncarton exec -- convert-pheno -help\n</code></pre>"},{"location":"usage/#method-2-from-cpan-in-a-conda-environment","title":"Method 2: From CPAN in a Conda environment","text":"<p>Please follow these instructions.</p>"},{"location":"usage/#method-3-from-github","title":"Method 3: From Github","text":"<p>To clone the repository for the first time:</p> <pre><code>git clone https://github.com/cnag-biomedical-informatics/convert-pheno.git\ncd convert-pheno\n</code></pre> <p>To update an existing clone, navigate to the repository folder and run:</p> <pre><code>git pull\n</code></pre> <p>Install system level dependencies:</p> <pre><code>sudo apt-get install cpanminus libbz2-dev zlib1g-dev libperl-dev libssl-dev\n</code></pre> <p>Now you have two choose between one of the 2 options below:</p> <p>Option 1: Install the dependencies at <code>~/perl5</code>:</p> <pre><code>cpanm --local-lib=~/perl5 local::lib &amp;&amp; eval $(perl -I ~/perl5/lib/perl5/ -Mlocal::lib)\ncpanm --notest --installdeps .\nbin/convert-pheno --help\n</code></pre> <p>To ensure Perl recognizes your local modules every time you start a new terminal, you should type:</p> <pre><code>echo 'eval $(perl -I ~/perl5/lib/perl5/ -Mlocal::lib)' &gt;&gt; ~/.bashrc\n</code></pre> <p>Option 2: Install the dependencies in a \"virtual environment\" (at <code>local/</code>) . We'll be using the module <code>Carton</code> for that:</p> <pre><code>mkdir local\ncpanm --notest --local-lib=local/ Carton\nexport PATH=$PATH:local/bin; export PERL5LIB=$(pwd)/local/lib/perl5:$PERL5LIB\ncarton install\ncarton exec -- bin/convert-pheno -help\n</code></pre>"},{"location":"usage/#containerized","title":"Containerized","text":""},{"location":"usage/#method-4-from-docker-hub","title":"Method 4: From Docker Hub","text":"<p>(Estimated Time: Approximately 15 seconds)</p> <p>Download the latest version of the Docker image (supports both amd64 and arm64 architectures) from Docker Hub by executing:</p> <pre><code>docker pull manuelrueda/convert-pheno:latest\ndocker image tag manuelrueda/convert-pheno:latest cnag/convert-pheno:latest\n</code></pre> <p>See additional instructions below.</p>"},{"location":"usage/#method-5-with-dockerfile","title":"Method 5: With Dockerfile","text":"<p>(Estimated Time: Approximately 3 minutes)</p> <p>Please download the <code>Dockerfile</code> from the repo:</p> <pre><code>wget https://raw.githubusercontent.com/cnag-biomedical-informatics/convert-pheno/main/Dockerfile\n</code></pre> <p>And then run:</p> <pre><code>docker buildx build -t cnag/convert-pheno:latest .\n</code></pre>"},{"location":"usage/#additional-instructions-for-methods-4-and-5","title":"Additional instructions for Methods 4 and 5","text":"<p>To run the container (detached) execute:</p> <pre><code>docker run -tid -e USERNAME=root --name convert-pheno cnag/convert-pheno:latest\n</code></pre> <p>To enter:</p> <pre><code>docker exec -ti convert-pheno bash\n</code></pre> <p>The command-line executable can be found at:</p> <pre><code>/usr/share/convert-pheno/bin/convert-pheno\n</code></pre> <p>The default container user is <code>root</code> but you can also run the container as <code>$UID=1000</code> (<code>dockeruser</code>). </p> <pre><code> docker run --user 1000 -tid --name convert-pheno cnag/convert-pheno:latest\n</code></pre> <p>Alternatively, you can use <code>make</code> to perform all the previous steps:</p> <pre><code>wget https://raw.githubusercontent.com/cnag-biomedical-informatics/convert-pheno/main/Dockerfile\nwget https://raw.githubusercontent.com/cnag-biomedical-informatics/convert-pheno/main/makefile.docker\nmake -f makefile.docker install\nmake -f makefile.docker run\nmake -f makefile.docker enter\n</code></pre>"},{"location":"usage/#mounting-volumes","title":"Mounting volumes","text":"<p>Docker containers are fully isolated. If you need the mount a volume to the container please use the following syntax (<code>-v host:container</code>).  Find an example below (note that you need to change the paths to match yours):</p> <pre><code>docker run -tid --volume /media/mrueda/4TBT/data:/data --name convert-pheno-mount cnag/convert-pheno:latest\n</code></pre> <p>Then I will do something like this:</p> <pre><code># First I create an alias to simplify invocation (from the host)\nalias convert-pheno='docker exec -ti convert-pheno-mount /usr/share/convert-pheno/bin/convert-pheno'\n\n# Now I use the alias to run the command (note that I use the flag --out-dir to specify the output directory)\nconvert-pheno -ibff /data/individuals.json -opxf pxf.json --out-dir /data\n</code></pre>"},{"location":"usage/#system-requirements","title":"System requirements","text":"<pre><code>- OS/ARCH supported: B&lt;linux/amd64&gt; and B&lt;linux/arm64&gt;.\n- Ideally a Debian-based distribution (Ubuntu or Mint), but any other (e.g., CentOS, OpenSUSE) should do as well (untested).\n  (It should also work on macOS and Windows Server, but we are only providing information for Linux here)\n* Perl 5 (&gt;= 5.26 core; installed by default in most Linux distributions). Check the version with \"perl -v\".\n* &gt;= 4GB of RAM\n* 1 core\n* At least 16GB HDD\n</code></pre>"},{"location":"usage/#how-to-run-convert-pheno","title":"HOW TO RUN CONVERT-PHENO","text":"<p>For executing convert-pheno you will need:</p> <ul> <li> <p>Input file(s):</p> <p>A text file in one of the accepted formats.</p> <p>Note that when using <code>--iomop</code>, I/O files can be gzipped.</p> </li> <li> <p>Optional: </p> <p>Athena-OHDSI database</p> <p>The database file is available at this link (~2.2GB). The database may be needed when using <code>-iomop</code>.</p> <p>Regardless if you're using the containerized or non-containerized version, the download procedure is the same. For CLI users, Google makes it difficult to use <code>wget</code>, <code>curl</code> or <code>aria2c</code> so we will use a <code>Python</code> module instead:</p> <pre><code>$ pip install gdown\n</code></pre> <p>And then run the following script</p> <pre><code>import gdown\n\nurl = 'https://drive.google.com/uc?export=download&amp;id=1-Ls1nmgxp-iW-8LkRIuNNdNytXa8kgNw'\noutput = './ohdsi.db'\ngdown.download(url, output, quiet=False)\n</code></pre> <p>Once downloaded, you have two options:</p> <p>a) Move the file <code>ohdsi.db</code> inside the <code>share/db/</code> directory.</p> <p>or</p> <p>b) Use the option <code>--path-to-ohdsi-db</code></p> </li> </ul> <p>Examples:</p> <p>Note that you can find input examples for all conversions within the <code>t/</code> directory of this repository:</p> <pre><code>$ bin/convert-pheno -ipxf t/pxf2bff/in/pxf.json -obff individuals.json\n\n$ bin/convert-pheno -ibff t/bff2pxf/in/individuals.json -opxf phenopackets.yaml --out-dir my_out_dir\n\n$ bin/convert-pheno -iomop t/omop2bff/in/omop_cdm_eunomia.sql -opxf phenopackets.json -max-lines-sql 2694\n\n$ bin/convert-pheno -ibff t/bff2omop/in/individuals.json -oomop my_prefix --ohdsi-db # Needs ohdsi.db\n</code></pre> <p>Generic examples:</p> <pre><code>$ $path/convert-pheno -iredcap redcap.csv -opxf phenopackets.json --redcap-dictionary redcap_dict.csv --mapping-file mapping_file.yaml\n\n$ $path/convert-pheno -iomop dump.sql.gz -obff individuals.json.gz --stream -omop-tables measurement -verbose\n\n$ $path/convert-pheno -cdisc cdisc_odm.xml -obff individuals.json --rcd redcap_dict.csv --mapping-file mapping_file.yaml --search mixed --min-text-similarity-score 0.6\n\n$ $path/convert-pheno -iomop *csv -obff individuals.json -sep ','\n\n$ carton exec -- $path/convert-pheno -ibff individuals.json -opxf phenopackets.json # If using Carton\n</code></pre>"},{"location":"usage/#common-errors-and-solutions","title":"COMMON ERRORS AND SOLUTIONS","text":"<pre><code>* Error message: CSV_XS ERROR: 2023 - EIQ - QUO character not allowed @ rec 1 pos 21 field 1\n  Solution: Make sure you use the right character separator for your data with --sep &lt;char&gt;. \n            The script tries to guess it from the file extension, but sometimes extension and actual separator do not match. \n            When using REDCap as input, make sure that &lt;--iredcap&gt; and &lt;--rcd&gt; files use the same separator field.\n            The defauly value for the separator is ';'. \n  Example for tab separator in CLI.\n   --sep  $'\\t'\n\n* Error message: Error: malformed UTF-8 character in JSON string, at character offses...\n  Solution:  iconv -f ISO-8859-1 -t UTF-8 yourfile.json -o yourfile-utf8.json\n</code></pre>"},{"location":"usage/#citation","title":"CITATION","text":"<p>The author requests that any published work that utilizes <code>Convert-Pheno</code> includes a cite to the the following reference:</p> <p>Rueda, M et al., (2024). Convert-Pheno: A software toolkit for the interconversion of standard data models for phenotypic data. Journal of Biomedical Informatics. DOI</p>"},{"location":"usage/#author","title":"AUTHOR","text":"<p>Written by Manuel Rueda, PhD. Info about CNAG can be found at https://www.cnag.eu.</p>"},{"location":"usage/#copyright-and-license","title":"COPYRIGHT AND LICENSE","text":"<p>This PERL file is copyrighted. See the LICENSE file included in this distribution.</p>"},{"location":"use-as-a-command-line-interface/","title":"\ud83d\udcbb As a command-line interface","text":"<p><code>Convert-Pheno</code> software includes a command-line utility, which is particularly useful when working with text files, such as those exported from PostgreSQL, as input.</p>"},{"location":"use-as-a-command-line-interface/#usage","title":"Usage","text":"<p>The operation is simple:</p> <pre><code>$ convert-pheno -input-format &lt;filein&gt; -output-format &lt;fileout&gt;\n</code></pre> Google Colab tutorial <p>We created a Google Colab tutorial notebook to enable testing <code>Convert-Pheno</code>in a virtual environment. Users can view notebooks shared publicly without sign-in, but you need a google account to execute code.</p> <p> </p> <p>Please see more examples in this README.</p> Inspiration <p>The command line operation was inspired by <code>convert</code> tool from ImageMagick and from OpenBabel.</p>"},{"location":"use-as-a-module/","title":"\ud83d\udce6 As a module","text":"<p><code>Convert-Pheno</code> core is a Perl module available at CPAN.</p>"},{"location":"use-as-a-module/#usage","title":"Usage","text":"<p>The module can be used within a <code>Perl</code> script, but it can also be utilized in scripts written in other languages, such as <code>Python</code>. </p> <pre><code>use Convert::Pheno;\n\nmy $my_pxf_json_data = {\n    \"phenopacket\" =&gt; {\n        \"id\"      =&gt; \"P0007500\",\n        \"subject\" =&gt; {\n            \"id\"          =&gt; \"P0007500\",\n            \"dateOfBirth\" =&gt; \"unknown-01-01T00:00:00Z\",\n            \"sex\"         =&gt; \"FEMALE\"\n        }\n    }\n};\n\n# Create object\nmy $convert = Convert::Pheno-&gt;new(\n    {\n        data   =&gt; $my_pxf_json_data,\n        method =&gt; 'pxf2bff'\n    }\n);\n\n# Apply a method\nmy $data = $convert-&gt;pxf2bff;\n</code></pre> Inside PerlInside Python <p>Find here an example script.</p> <p>Find here an example script. </p> <ul> <li>It should work out of the box with the containerized version. </li> <li>You also have instructions in how to run it in a conda environment.</li> </ul> <p>Perl inside Python, is that even possible ?</p> <p>Perl easily integrates with other languages and allows for embedding them into Perl code (e.g., using <code>Inline</code>). However, embedding Perl code into other languages is not as simple. Fortunately, the PyPerler library provides a solution for this issue.</p>"},{"location":"use-as-an-api/","title":"\ud83d\udd17 As an API","text":"<p>In some cases, using an API for sending and receiving data as a microservice may be more efficient. To address this, we have developed a lightweight REST API that allows for sending <code>POST</code> requests and receiving <code>JSON</code> responses</p>"},{"location":"use-as-an-api/#usage","title":"Usage","text":"<p>Just make sure to send your <code>POST</code> data in the proper format. </p> <pre><code>curl -d \"@data.json\" -H 'Content-Type: application/json' -X POST http://localhost:3000/api\n</code></pre> <p>where <code>data.json</code> looks like the below:</p> <pre><code>{\n \"data\": {...}\n \"method\": \"pxf2bff\"\n}\n</code></pre> Interactive API specification <p>Please find here interactive documentation (built with ReDoc).</p>"},{"location":"use-as-an-api/#included-apis","title":"Included APIs","text":"<p>We included two flavours of the same API, one in <code>Perl</code> and another in <code>Python</code>.Both APIs were created by using OpenAPI 3.0.2 schema and should work out of the box with the containerized version.</p> <p>Local or remote installation?</p> <p>The API should be installed on a local server.</p> Perl versionPython version <p>Please see more detailed instructions at this README.</p> <p>Please see more detailed instructions at this README.</p>"},{"location":"what-is-convert-pheno/","title":"What is Convert-Pheno?","text":"<p><code>Convert-Pheno</code> is an open-source toolkit designed to interconvert common phenotypic data models, facilitating easier data sharing and integration across different standards.</p> <p> </p> Convert-Pheno schematic view"},{"location":"what-is-convert-pheno/#basic-usage-command-line-interface","title":"Basic Usage: Command-Line Interface","text":"<p>Most users interact with <code>Convert-Pheno</code> via its Command-Line Interface (CLI). The CLI operates directly on text files, providing straightforward input-output interaction.</p>"},{"location":"what-is-convert-pheno/#advanced-usage-alternative-operation-modes","title":"Advanced Usage: Alternative Operation Modes","text":"<p><code>Convert-Pheno</code> also supports several alternative modes of operation to accommodate diverse user needs:</p> <ul> <li>Module: Embed <code>Convert-Pheno</code> directly into your custom scripts or automated pipelines.</li> <li>API Access: Programmatically access conversion functionalities through a standardized API.</li> </ul>"},{"location":"what-is-convert-pheno/#web-application-interface","title":"Web Application Interface","text":"<ul> <li>Web Application Interface: An intuitive and interactive web-based user interface.</li> </ul>"},{"location":"what-is-convert-pheno/#listen-to-the-paper-audio-edition","title":"Listen to the Paper: Audio Edition","text":"<p>Podcast-Style Audio Format</p> <p>Explore the key insights of this paper in audio format! Perfect for learning on the go or through immersive narration.</p> <p>    Your browser does not support the audio element. </p> <p>Made with Notebook LM</p>"},{"location":"md/data-validation/","title":"Data validation","text":"<p>Data validation</p> <p>To ensure the integrity and validity of converted outputs, we employ external validation tools during development and in unit tests. Specifically, we used the bff-tools validate for Beacon Friendly Format (BFF) and phenopacket-tools for Phenotype Exchange Format (PXF). These validators were instrumental in ensuring converted data adhere to the respective schemas and standards; for example, conversions were validated until the output was 100% compliant with the target schema. The same validation process is applied to Beacon v2 and OMOP CDM outputs. By preserving non-mapped variables where appropriate and applying rigorous validation, we aim to mitigate information loss and maximise fidelity of the converted data.</p> <p>Important: Convert-Pheno does not validate your input data. Input validation is out of scope for the software. If fields are missing or malformed, Convert-Pheno will handle these cases internally and apply default values where appropriate, but it will not verify that your source files are complete or correct. We therefore recommend validating and cleaning source files before conversion.</p> <p>See:</p> <ul> <li>bff-tools validate</li> <li>phenopacket-tools</li> <li>OMOP CSV Validator</li> </ul>"},{"location":"tbl/db-search/","title":"Db search","text":"About text similarity in database searches <p><code>Convert-Pheno</code> comes with several pre-configured ontology/terminology databases. It supports three types of label-based search strategies:</p> Composite Similarity Score <p>The composite similarity score is computed as a weighted sum of two measures: the token-based similarity and the normalized Levenshtein similarity.</p>"},{"location":"tbl/db-search/#1-exact-default","title":"1. <code>exact</code> (default)","text":"<p>Returns only exact matches for the given label string. If the label is not found exactly, no results are returned.</p>"},{"location":"tbl/db-search/#2-mixed-use-search-mixed","title":"2. <code>mixed</code> (use <code>--search mixed</code>)","text":"<p>Hybrid search: First tries to find an exact label match. If none is found, it performs a token-based similarity search and returns the closest matching concept based on the highest similarity score.</p>"},{"location":"tbl/db-search/#3-fuzzy-use-search-fuzzy","title":"3. \u2728 <code>fuzzy</code> (use <code>--search fuzzy</code>)","text":"<p>Hybrid search with fuzzy ranking: Like <code>mixed</code>, it starts with an exact match attempt. If that fails, it performs a weighted similarity search, where: - 90% of the score comes from token-based similarity (e.g., cosine or Dice coefficient), - 10% comes from the normalized Levenshtein similarity.</p> <p>The concept with the highest composite score is returned.</p> <p>Note: The normalized Levenshtein similarity is computed on top of the candidate results produced by the full text search. In this approach, an initial full text search (using token-based methods) returns a set of potential matches. The fuzzy search then refines these results by applying the normalized Levenshtein distance to better handle minor typographical differences, ensuring that the final composite score reflects both overall token similarity and fine-grained character-level differences.</p>"},{"location":"tbl/db-search/#example-search-behavior","title":"\ud83d\udd0d Example Search Behavior","text":"<p>Query: <code>Exercise pain management</code> - With <code>--search exact</code>: \u2705 Match found \u2014 Exercise Pain Management</p> <p>Query: <code>Brain Hemorrhage</code> - With <code>--search mixed</code>:   - \u274c No exact match   - \u2705 Closest match by similarity: Intraventricular Brain Hemorrhage</p>"},{"location":"tbl/db-search/#similarity-threshold","title":"\ud83d\udca1 Similarity Threshold","text":"<p>The <code>--min-text-similarity-score</code> option sets the minimum threshold for <code>mixed</code> and <code>fuzzy</code> searches. - Default: <code>0.8</code> (conservative) - Lowering the threshold may increase recall but may introduce irrelevant matches.</p>"},{"location":"tbl/db-search/#performance-note","title":"\u26a0\ufe0f Performance Note","text":"<p>Both <code>mixed</code> and <code>fuzzy</code> modes are more computationally intensive and can produce unexpected or less interpretable matches. Use them with care, especially on large datasets.</p>"},{"location":"tbl/db-search/#example-results-table","title":"\ud83e\uddea Example Results Table","text":"<p>Below is an example showing how the query <code>Sudden Death Syndrome</code> performs using different search modes against the NCIt ontology:</p> Query Search NCIt match (label) NCIt code Cosine Dice Levenshtein (Normalized) Composite Sudden Death Syndrome exact NA NA NA NA NA NA mixed CDISC SDTM Sudden Death Syndrome Type Terminology NCIT:C101852 0.65 0.60 NA NA Family History of Sudden Arrythmia Death Syndrome NCIT:C168019 0.65 0.60 NA NA Family History of Sudden Infant Death Syndrome NCIT:C168209 0.65 0.60 NA NA Sudden Infant Death Syndrome NCIT:C85173 0.86 0.86 NA NA \u2728 fuzzy CDISC SDTM Sudden Death Syndrome Type Terminology NCIT:C101852 0.65 0.60 0.43 0.63 Family History of Sudden Arrythmia Death Syndrome NCIT:C168019 0.65 0.60 0.43 0.63 Family History of Sudden Infant Death Syndrome NCIT:C168209 0.65 0.60 0.46 0.63 Sudden Infant Death Syndrome NCIT:C85173 0.86 0.86 0.75 0.85 <p>Interpretation: </p> <ul> <li> <p>With <code>exact</code>, there are no matches.</p> </li> <li> <p>With <code>mixed</code>, the best match will be <code>Sudden Infant Death Syndrome</code>.</p> </li> <li> <p>With <code>fuzzy</code>, the composite score (90% token-based + 10% Levenshtein similarity) is used to rank results.   The highest match is <code>Sudden Infant Death Syndrome</code>, with a composite score of 0.85.</p> </li> </ul> <p>\u2728 Now we introduce a typo on the query <code>Sudden Infant Deth Syndrome</code>:</p> Query Mode Candidate Label Code Cosine Dice Levenshtein (Normalized) Composite Sudden Infant Deth Syndrome fuzzy CDISC SDTM Sudden Death Syndrome Type Terminology NCIT:C101852 0.38 0.36 0.33 0.37 Family History of Sudden Arrythmia Death Syndrome NCIT:C168019 0.38 0.36 0.43 0.38 Family History of Sudden Infant Death Syndrome NCIT:C168209 0.57 0.55 0.59 0.57 Sudden Infant Death Syndrome NCIT:C85173 0.75 0.75 0.96 0.77 <p>To capture the best match we would need to lower the threshold to  <code>--min-text-similarity-score 0.75</code></p> <p>It is possible to change the weight of Levenshtein similarity via <code>--levenshtein-weight &lt;floating 0.0 - 1.0&gt;</code>.</p>"},{"location":"tbl/db-search/#1-token-based-similarity","title":"1. Token-Based Similarity","text":"<p>This is calculated using methods like cosine or Dice similarity to measure how similar the tokens (words) of two strings are.</p>"},{"location":"tbl/db-search/#2-normalized-levenshtein-similarity","title":"2. Normalized Levenshtein Similarity","text":"<p>The normalized Levenshtein similarity is defined as:</p> \\[ \\text{NormalizedLevenshtein}(s_1, s_2) = 1 - \\frac{\\text{lev}(s_1, s_2)}{\\max(|s_1|, |s_2|)} \\] <p>Where: - \\(\\text{lev}(s_1, s_2)\\) is the Levenshtein edit distance\u2014the minimum number of insertions, deletions, or substitutions required to change \\(s_1\\) into \\(s_2\\). - \\(|s_1|\\) and \\(|s_2|\\) are the lengths of the strings \\(s_1\\) and \\(s_2\\), respectively.</p> <p>This formula produces a score between 0 and 1, with 1.0 meaning identical strings and 0.0 meaning completely different strings.</p>"},{"location":"tbl/db-search/#3-composite-score-formula","title":"3. Composite Score Formula","text":"<p>The final composite similarity score \\(C\\) is a weighted combination of the two metrics:</p> \\[ C(s_1, s_2) = \\alpha \\cdot \\text{TokenSimilarity}(s_1, s_2) + \\beta \\cdot \\text{NormalizedLevenshtein}(s_1, s_2) \\] <p>Where: - \\(\\alpha\\) (or <code>token_weight</code>) is the weight assigned to the token-based similarity. - \\(\\beta\\) (or <code>lev_weight</code>) is the weight assigned to the normalized Levenshtein similarity.</p> <p>A common default is to set \\(\\alpha = 0.9\\) and \\(\\beta = 0.1\\), emphasizing the token-based similarity. However, for short strings (4\u20135 words), you might consider adjusting the balance (for example, \\(\\alpha = 0.95\\) and \\(\\beta = 0.05\\)) if small typographical differences are less critical.</p>"},{"location":"tbl/formats/","title":"Formats","text":"Input CLI UI Module API Beacon v2 Models YES YES YES YES CDISC-ODM YES YES YES NO CSV YES NO YES NO Phenopackets v2 YES YES YES YES OMOP-CDM YES YES YES YES REDCap YES YES YES NO"},{"location":"tbl/mapping-bff2pxf/","title":"Mapping bff2pxf","text":""},{"location":"tbl/mapping-bff2pxf/#version-025","title":"Version 0.25","text":""},{"location":"tbl/mapping-bff2pxf/#terms","title":"Terms","text":""},{"location":"tbl/mapping-bff2pxf/#id","title":"id","text":"BFF JSON path PXF JSON path (UNIQUE ID) id"},{"location":"tbl/mapping-bff2pxf/#subject","title":"subject","text":"BFF JSON path PXF JSON path id subject.id (ALIVE) subject.vitalStatus sex.label subject.sex info.dateOfBirth subject.dateOfBirth karyotypicSex subject.karyotypicSex"},{"location":"tbl/mapping-bff2pxf/#phenotypicfeatures","title":"phenotypicFeatures","text":"BFF JSON path PXF JSON path phenotypicFeatures.featureType phenotypicFeatures.type phenotypicFeatures.excluded phenotypicFeatures.excluded"},{"location":"tbl/mapping-bff2pxf/#measurements","title":"measurements","text":"BFF JSON path PXF JSON path measures.assayCode measurements.assay measures.measurementValue measurements.value measures.measurementValue.typedQuantities.quantityType measurements.complexValue.typedQuantities.type"},{"location":"tbl/mapping-bff2pxf/#biosamples","title":"biosamples","text":"BFF JSON path PXF JSON path info.biosamples biosamples"},{"location":"tbl/mapping-bff2pxf/#interpretations","title":"interpretations","text":"BFF JSON path PXF JSON path info.interpretations interpretations"},{"location":"tbl/mapping-bff2pxf/#diseases","title":"diseases","text":"BFF JSON path PXF JSON path diseases.diseaseCode diseases.term diseases.ageOfOnset diseases.onset"},{"location":"tbl/mapping-bff2pxf/#medicalactions","title":"medicalActions","text":"BFF JSON path PXF JSON path interventionsOrProcedures.procedureCode medicalActions.procedure.code interventionsOrProcedures.ageAtProcedure medicalActions.procedure.performed treatments.treatmentCode medicalActions.treatment.agent"},{"location":"tbl/mapping-bff2pxf/#files","title":"files","text":"<p>NA</p>"},{"location":"tbl/mapping-bff2pxf/#metadata","title":"metaData","text":"BFF JSON path PXF JSON path info.metaData metaData"},{"location":"tbl/mapping-bff2pxf/#exposures-not-listed-in-pxf-documentation","title":"exposures (not-listed in PXF documentation)","text":"BFF JSON path PXF JSON path exposures.exposureCode exposures.type exposures.date exposures.occurrence.timestamp"},{"location":"tbl/mapping-file/","title":"Mapping file","text":"What is a <code>Convert-Pheno</code> mapping file? <p>A mapping file is a text file in YAML format (JSON is also accepted) that connects a set of variables to a format that is understood by <code>Convert-Pheno</code>. This file maps your variables to the required terms of the individuals entity from the Beacon v2 models, which serves a center model.</p>"},{"location":"tbl/mapping-file/#creating-a-mapping-file","title":"Creating a mapping file","text":"<p>To create a mapping file, start by reviewing the example mapping file provided with the installation. The goal is to replace the contents of such file with those from your REDCap project. The mapping file contains the following types of data:</p> Type Required (Optional) Required properties Optional properties Internal <code>project</code> <code>id, source, ontology, version</code> <code>description, baselineFieldsToPropagate</code> Beacon v2 terms <code>id, sex (diseases, exposures, info, interventionsOrProcedures, measures, phenotypicFeatures, treatments)</code> <code>fields</code> <code>age,ageOfOnset,assignTermIdFromHeader,bodySite,dateOfProcedure,dictionary,drugDose,drugUnit,duration,durationUnit,familyHistory,fields,mapping,procedureCodeLabel,selector,terminology,unit,visitId</code> <p>These are the properties needed to map your data to the entity <code>individuals</code> in the Beacon v2 Models:</p> <ul> <li>baselineFieldsToPropagate, an array of columns containing measurements that were taken only at the initial time point (time = 0). Use this if you wish to duplicate these columns across subsequent rows for the same patient ID. It is important to ensure that the row containing baseline information appears first in the CSV.</li> <li>age, a <code>string</code> representing the column that points to the age of the patient.</li> <li>ageOfOnset, an <code>object</code> representing the column that points to the age at which the patient first experienced symptoms or was diagnosed with a condition.</li> <li>assignTermIdFromHeader, an <code>array</code> for columns on which the ontology-term ids have to be assigned from the header.</li> <li>bodySite, an <code>object</code> representing the column that points to the part of the body affected by a condition or where a procedure was performed.</li> <li>dateOfProcedure, an <code>object</code> representing the column that points to when a procedure took place.</li> <li>dictionary, is an <code>object</code> in the form of <code>key: value</code>. The <code>key</code> represents the original variable name in REDCap and the <code>value</code> represents the \"phrase\" that will be used to query a database to find an ontology candidate. For instance, you may have a variable named <code>cigarettes_days</code>, but you know that in NCIt the label is <code>Average Number Cigarettes Smoked a Day</code>. In this case, you will use <code>cigarettes_days: Average Number Cigarettes Smoked a Day</code>.</li> <li>drugDose, an <code>object</code> representing the column that points to the dose column for each treatment.</li> <li>drugUnit, an <code>object</code> representing the column that points to the unit column for each treatment.</li> <li>duration, an <code>object</code> representing the column that points to the duration column for each treatment.</li> <li>durationUnit, an <code>object</code> representing the column that points to the duration unit column for each treatment.</li> <li>familyHistory, an <code>object</code> representing the column that points to the family medical history relevant to the patient's condition.</li> <li>fields, can be either a <code>string</code> or an <code>array</code> consisting of the name of the REDCap variables that map to that Beacon v2 term.</li> <li>mapping, is an <code>object</code> in the form of <code>key: value</code> that we use to map our Beacon v2 objects to REDCap variables.</li> <li>procedureCodeLabel , a nested <code>object</code> with specific mappings for <code>interventionsOrProcedures</code>.</li> <li>ontology, it's an <code>string</code> to define more granularly the ontology for this particular Beacon v2 term. If not present, the script will use that from <code>project.ontology</code>.</li> <li>routeOfAdministration, a nested <code>object</code> with specific mappings for <code>treatments</code>.</li> <li>selector, a nested <code>object</code> value with specific mappings.</li> <li>terminology, a nested <code>object</code> value with user-defined ontology terms.</li> </ul> Terminology example <pre><code>terminology:\n  My fav term:\n    id: FOO:12345678\nlabel: Label for my fav term\n</code></pre> <ul> <li>unit, an <code>object</code> representing the column that points to the unit of measurement for a given value or treatment.</li> <li>visitId, the column with visit occurrence id.</li> </ul> Defining the values in the property <code>dictionary</code> <p>Before assigning values to <code>dictionary</code> it's important that you think about which ontologies/terminologies you want to use. The field <code>project.ontology</code> defines the ontology for the whole project, but you can also specify a another antology at the Beacon v2 term level. Once you know which ontologies to use, then try searching for such term to get an accorate label for it. For example, if you have chosen <code>ncit</code>, you can search for the values within NCIt at EBI Search. <code>Convert-Pheno</code> will use these values to retrieve the actual ontology term from its internal databases.</p>"},{"location":"tbl/mapping-omop2bff/","title":"Mapping omop2bff","text":""},{"location":"tbl/mapping-omop2bff/#version-025","title":"Version 0.25","text":""},{"location":"tbl/mapping-omop2bff/#terms","title":"Terms","text":""},{"location":"tbl/mapping-omop2bff/#diseases","title":"diseases","text":"OMOP Table(s) OMOP Variable BFF JSON path CONDITION_OCCURRENCE, PERSON condition_start_date, birth_datetime diseases.ageOfOnset CONDITION_OCCURRENCE condition_concept_id diseases.diseaseCode CONDITION_OCCURRENCE All variables diseases._info CONDITION_OCCURRENCE condition_status_concept_id diseases.stage CONDITION_OCCURRENCE person_id, visit_occurrence_id diseases._visit"},{"location":"tbl/mapping-omop2bff/#ethnicity","title":"ethnicity","text":"OMOP Table(s) OMOP Variable BFF JSON path PERSON race_source_value ethnicity"},{"location":"tbl/mapping-omop2bff/#exposures","title":"exposures","text":"OMOP Table(s) OMOP Variable BFF JSON path OBSERVATION, PERSON observation_date, birth_datetime exposures.ageAtExposure OBSERVATION observation_date exposures.date DEFAULT exposures.duration OBSERVATION All variables exposures._info OBSERVATION observation_concept_id exposures.exposureCode OBSERVATION unit_concept_id exposures.unit OBSERVATION value_as_number exposures.value"},{"location":"tbl/mapping-omop2bff/#geographicorigin","title":"geographicOrigin","text":"OMOP Table(s) OMOP Variable BFF JSON path PERSON ethnicity_source_value geographicOrigin"},{"location":"tbl/mapping-omop2bff/#id","title":"id","text":"OMOP Table(s) OMOP Variable BFF JSON path PERSON person_id id"},{"location":"tbl/mapping-omop2bff/#info","title":"info","text":"OMOP Table(s) OMOP Variable BFF JSON path PERSON All variables info.PERSON PERSON birth_datetime info.dateOfBirth"},{"location":"tbl/mapping-omop2bff/#interventionsorprocedures","title":"interventionsOrProcedures","text":"OMOP Table(s) OMOP Variable BFF JSON path PROCEDURE_OCCURRENCE, PERSON procedure_date, birth_datetime interventionsOrProcedures.ageAtProcedure PROCEDURE_OCCURRENCE procedure_date interventionsOrProcedures.dateOfProcedure PROCEDURE_OCCURRENCE All variables interventionsOrProcedures._info PROCEDURE_OCCURRENCE procedure_concept_id interventionsOrProcedures.procedureCode"},{"location":"tbl/mapping-omop2bff/#karyotypicsex","title":"karyotypicSex","text":"<p>NA</p>"},{"location":"tbl/mapping-omop2bff/#measures","title":"measures","text":"OMOP Table(s) OMOP Variable BFF JSON path MEASUREMENT measurement_concept_id measures.assayCode MEASUREMENT measurement_date measures.date MEASUREMENT value_as_concept_id measures.measurementValue MEASUREMENT unit_concept_id measures.measurementValue.quantity.unit MEASUREMENT value_as_number measures.measurementValue.quantity.value MEASUREMENT operator_concept_id, value_as_number, unit_concept_id measures.measurementValue.quantity.referenceRange MEASUREMENT All variables measures._info MEASUREMENT, PERSON measurement_date, birth_datetime measures.observationMoment MEASUREMENT measurement_date, birth_datetime measures.procedure.ageAtProcedure MEASUREMENT measurement_date measures.procedure.dateOfProcedure MEASUREMENT measurement_type_concept_id measures.procedure.prodecureCode MEASUREMENT person_id, visit_occurrence_id diseases._visit"},{"location":"tbl/mapping-omop2bff/#pedigrees","title":"pedigrees","text":"<p>NA</p>"},{"location":"tbl/mapping-omop2bff/#phenotypicfeatures","title":"phenotypicFeatures","text":"OMOP Table(s) OMOP Variable BFF JSON path OBSERVATION observation_concept_id phenotypicFeatures.featureType OBSERVATION All variables phenotypicFeatures._info OBSERVATION, PERSON observation_date, birth_datetime phenotypicFeatures.onset OBSERVATION person_id, visit_occurrence_id phenotypicFeatures._visit"},{"location":"tbl/mapping-omop2bff/#sex","title":"sex","text":"OMOP Table(s) OMOP Variable BFF JSON path PERSON gender_concept_id sex"},{"location":"tbl/mapping-omop2bff/#treatments","title":"treatments","text":"OMOP Table(s) OMOP Variable BFF JSON path DRUG_EXPOSURE, PERSON drug_exposure_start_date, birth_datetime treatments.ageOfOnset DEFAULT treatments.date DRUG_EXPOSURE All variables treatments._info DEFAULT treatments.doseIntervals DEFAULT treatments.routeOfAdministration DRUG_EXPOSURE drug_concept_id treatments.treatmentCode DRUG_EXPOSURE person_id, visit_occurrence_id treatments._visit"},{"location":"tbl/mapping-pxf2bff/","title":"Mapping pxf2bff","text":""},{"location":"tbl/mapping-pxf2bff/#version-025","title":"Version 0.25","text":""},{"location":"tbl/mapping-pxf2bff/#terms","title":"Terms","text":""},{"location":"tbl/mapping-pxf2bff/#diseases","title":"diseases","text":"PXF JSON path BFF JSON path diseases diseases diseases.term diseases.diseaseCode diseases.onset diseases.ageOfOnset"},{"location":"tbl/mapping-pxf2bff/#ethnicity","title":"ethnicity","text":"<p>NA</p>"},{"location":"tbl/mapping-pxf2bff/#exposures","title":"exposures","text":"PXF JSON path BFF JSON path exposures exposures exposures.type exposures.exposureCode exposures.occurrence.timestamp exposures.date"},{"location":"tbl/mapping-pxf2bff/#geographicorigin","title":"geographicOrigin","text":"<p>NA</p>"},{"location":"tbl/mapping-pxf2bff/#id","title":"id","text":"PXF JSON path BFF JSON path subject.id id"},{"location":"tbl/mapping-pxf2bff/#info","title":"info","text":"PXF JSON path BFF JSON path dateOfBirth, genes, metaData, variants, interpretations, files, biosample info"},{"location":"tbl/mapping-pxf2bff/#interventionsorprocedures","title":"interventionsOrProcedures","text":"PXF JSON path BFF JSON path medicalActions.procedure interventionsOrProcedures medicalActions.procedure.code interventionsOrProcedures.procedureCode medicalActions.procedure.performed interventionsOrProcedures.ageAtProcedure"},{"location":"tbl/mapping-pxf2bff/#karyotypicsex","title":"karyotypicSex","text":"PXF JSON path BFF JSON path subject.karyotypicSex karyotypicSex"},{"location":"tbl/mapping-pxf2bff/#measures","title":"measures","text":"PXF JSON path BFF JSON path measurements measures measurements.assay measures.assayCode measurements.value measures.measurementValue measurements.complexValue.typedQuantities.type measures.measurementValue.typedQuantities.quantityType measurements.timeObserved measures.observationMoment measurements.procedure measures.procedure"},{"location":"tbl/mapping-pxf2bff/#pedigrees","title":"pedigrees","text":"<p>NA</p>"},{"location":"tbl/mapping-pxf2bff/#phenotypicfeatures","title":"phenotypicFeatures","text":"PXF JSON path BFF JSON path phenotypicFeatures phenotypicFeatures phenotypicFeatures.type phenotypicFeatures.featureType phenotypicFeatures.negated phenotypicFeatures.excluded phenotypicFeatures.evidence (array) phenotypicFeatures.evidence (v2.0.0 is still object)"},{"location":"tbl/mapping-pxf2bff/#sex","title":"sex","text":"PXF JSON path BFF JSON path subject.sex sex"},{"location":"tbl/mapping-pxf2bff/#treatments","title":"treatments","text":"PXF JSON path BFF JSON path medicalActions.treatment treatments medicalActions.treatment.agent treatments.treatmentCode"}]}